# SOME DESCRIPTIVE TITLE
# Copyright (C) YEAR Free Software Foundation, Inc.
# This file is distributed under the same license as the PACKAGE package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"POT-Creation-Date: 2023-09-03 08:14+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#. This guide is maintained in the main Quarkus repository
#. and pull requests should be submitted there:
#. https://github.com/quarkusio/quarkus/tree/main/docs/src/main/asciidoc
#. type: Title =
#: upstream/_versions/3.2/guides/kafka.adoc:6
#, no-wrap
msgid "Apache Kafka Reference Guide"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:15
msgid "This reference guide demonstrates how your Quarkus application can utilize SmallRye Reactive Messaging to interact with Apache Kafka."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:16
#, no-wrap
msgid "Introduction"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:21
msgid "https://kafka.apache.org[Apache Kafka] is a popular open-source distributed event streaming platform.  It is used commonly for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.  Similar to a message queue, or an enterprise messaging platform, it lets you:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:23
#, no-wrap
msgid "*publish* (write) and *subscribe* to (read) streams of events, called _records_.\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:24
#, no-wrap
msgid "*store* streams of records durably and reliably inside _topics_.\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:25
#, no-wrap
msgid "*process* streams of records as they occur or retrospectively.\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:27
msgid "And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:28
#, no-wrap
msgid "Quarkus Extension for Apache Kafka"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:32
msgid "Quarkus provides support for Apache Kafka through https://smallrye.io/smallrye-reactive-messaging/[SmallRye Reactive Messaging] framework.  Based on Eclipse MicroProfile Reactive Messaging specification 2.0, it proposes a flexible programming model bridging CDI and event-driven."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:37
msgid "This guide provides an in-depth look on Apache Kafka and SmallRye Reactive Messaging framework.  For a quick start take a look at xref:kafka-reactive-getting-started.adoc[Getting Started to SmallRye Reactive Messaging with Apache Kafka]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:40
msgid "You can add the `smallrye-reactive-messaging-kafka` extensions to your project by running the following command in your project base directory:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:45
msgid "This will add the following to your build file:"
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:47
#: upstream/_versions/3.2/guides/kafka.adoc:1651
#: upstream/_versions/3.2/guides/kafka.adoc:1900
#: upstream/_versions/3.2/guides/kafka.adoc:1968
#, no-wrap
msgid "pom.xml"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:53
#, no-wrap
msgid ""
"<dependency>\n"
"    <groupId>io.quarkus</groupId>\n"
"    <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>\n"
"</dependency>\n"
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:56
#: upstream/_versions/3.2/guides/kafka.adoc:1660
#: upstream/_versions/3.2/guides/kafka.adoc:1909
#: upstream/_versions/3.2/guides/kafka.adoc:1978
#, no-wrap
msgid "build.gradle"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:59
#, no-wrap
msgid "implementation(\"io.quarkus:quarkus-smallrye-reactive-messaging-kafka\")\n"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:64
msgid "The extension includes `kafka-clients` version 3.2.1 as a transitive dependency and is compatible with Kafka brokers version 2.x."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:66
#, no-wrap
msgid "Configuring Smallrye Kafka Connector"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:69
msgid "Because Smallrye Reactive Messaging framework supports different messaging backends like Apache Kafka, AMQP, Apache Camel, JMS, MQTT, etc., it employs a generic vocabulary:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:71
msgid "Applications send and receive *messages*. A message wraps a _payload_ and can be extended with some _metadata_. With the Kafka connector, a _message_ corresponds to a Kafka _record_."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:72
msgid "Messages transit on *channels*. Application components connect to channels to publish and consume messages. The Kafka connector maps _channels_ to Kafka _topics_."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:73
msgid "Channels are connected to message backends using *connectors*. Connectors are configured to map incoming messages to a specific channel (consumed by the application) and collect outgoing messages sent to a specific channel. Each connector is dedicated to a specific messaging technology. For example, the connector dealing with Kafka is named `smallrye-kafka`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:75
msgid "A minimal configuration for the Kafka connector with an incoming channel looks like the following:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:80
#, no-wrap
msgid ""
"%prod.kafka.bootstrap.servers=kafka:9092 <1>\n"
"mp.messaging.incoming.prices.connector=smallrye-kafka <2>\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:84
msgid "Configure the broker location for the production profile. You can configure it globally or per channel using `mp.messaging.incoming.$channel.bootstrap.servers` property.  In dev mode and when running tests, xref:kafka-dev-services[Dev Services for Kafka] automatically starts a Kafka broker.  When not provided this property defaults to `localhost:9092`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:85
msgid "Configure the connector to manage the prices channel. By default, the topic name is same as the channel name. You can configure the topic attribute to override it."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:87
msgid "The `%prod` prefix indicates that the property is only used when the application runs in prod mode (so not in dev or test). Refer to the xref:config-reference.adoc#profiles[Profile documentation] for further details."
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:89
#, no-wrap
msgid "Connector auto-attachment"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:94
msgid "If you have a single connector on your classpath, you can omit the `connector` attribute configuration.  Quarkus automatically associates _orphan_ channels to the (unique) connector found on the classpath.  _Orphans_ channels are outgoing channels without a downstream consumer or incoming channels without an upstream producer."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:96
msgid "This auto-attachment can be disabled using:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:100
#, no-wrap
msgid "quarkus.reactive-messaging.auto-connector-attachment=false\n"
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:103
#, no-wrap
msgid "Receiving messages from Kafka"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:106
msgid "Continuing from the previous minimal configuration, your Quarkus application can receive message payload directly:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:110
#: upstream/_versions/3.2/guides/kafka.adoc:2438
#: upstream/_versions/3.2/guides/kafka.adoc:2519
#, no-wrap
msgid "import org.eclipse.microprofile.reactive.messaging.Incoming;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:112
#: upstream/_versions/3.2/guides/kafka.adoc:729
#: upstream/_versions/3.2/guides/kafka.adoc:1206
#: upstream/_versions/3.2/guides/kafka.adoc:1251
#: upstream/_versions/3.2/guides/kafka.adoc:1322
#: upstream/_versions/3.2/guides/kafka.adoc:1345
#: upstream/_versions/3.2/guides/kafka.adoc:1387
#: upstream/_versions/3.2/guides/kafka.adoc:1557
#, no-wrap
msgid "import jakarta.enterprise.context.ApplicationScoped;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:115
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class PriceConsumer {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:120
#, no-wrap
msgid ""
"    @Incoming(\"prices\")\n"
"    public void consume(double price) {\n"
"        // process your price.\n"
"    }\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:122
#: upstream/_versions/3.2/guides/kafka.adoc:268
#: upstream/_versions/3.2/guides/kafka.adoc:596
#: upstream/_versions/3.2/guides/kafka.adoc:622
#: upstream/_versions/3.2/guides/kafka.adoc:914
#: upstream/_versions/3.2/guides/kafka.adoc:1274
#: upstream/_versions/3.2/guides/kafka.adoc:1335
#: upstream/_versions/3.2/guides/kafka.adoc:1363
#: upstream/_versions/3.2/guides/kafka.adoc:1415
#: upstream/_versions/3.2/guides/kafka.adoc:1518
#: upstream/_versions/3.2/guides/kafka.adoc:1576
#: upstream/_versions/3.2/guides/kafka.adoc:1963
#: upstream/_versions/3.2/guides/kafka.adoc:2043
#: upstream/_versions/3.2/guides/kafka.adoc:2426
#: upstream/_versions/3.2/guides/kafka.adoc:2506
#: upstream/_versions/3.2/guides/kafka.adoc:2539
#, no-wrap
msgid "}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:125
msgid "There are several other ways your application can consume incoming messages:"
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:126
#, no-wrap
msgid "Message"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:138
#, no-wrap
msgid ""
"@Incoming(\"prices\")\n"
"public CompletionStage<Void> consume(Message<Double> msg) {\n"
"    // access record metadata\n"
"    var metadata = msg.getMetadata(IncomingKafkaRecordMetadata.class).orElseThrow();\n"
"    // process the message payload.\n"
"    double price = msg.getPayload();\n"
"    // Acknowledge the incoming message (commit the offset)\n"
"    return msg.ack();\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:142
msgid "The `Message` type lets the consuming method access the incoming message metadata and handle the acknowledgment manually.  We'll explore different acknowledgment strategies in xref:commit-strategies[Commit Strategies]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:144
msgid "If you want to access the Kafka record objects directly, use:"
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:145
#, no-wrap
msgid "ConsumerRecord"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:156
#, no-wrap
msgid ""
"@Incoming(\"prices\")\n"
"public void consume(ConsumerRecord<String, Double> record) {\n"
"    String key = record.key(); // Can be `null` if the incoming record has no key\n"
"    String value = record.value(); // Can be `null` if the incoming record has no value\n"
"    String topic = record.topic();\n"
"    int partition = record.partition();\n"
"    // ...\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:160
msgid "`ConsumerRecord` is provided by the underlying Kafka client and can be injected directly to the consumer method.  Another simpler approach consists in using `Record`:"
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:161
#, no-wrap
msgid "Record"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:169
#, no-wrap
msgid ""
"@Incoming(\"prices\")\n"
"public void consume(Record<String, Double> record) {\n"
"    String key = record.key(); // Can be `null` if the incoming record has no key\n"
"    String value = record.value(); // Can be `null` if the incoming record has no value\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:172
msgid "`Record` is a simple wrapper around key and payload of the incoming Kafka record."
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:173
#, no-wrap
msgid "@Channel"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:176
msgid "Alternatively, your application can inject a `Multi` in your bean and subscribe to its events as the following example:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:181
#, no-wrap
msgid ""
"import io.smallrye.mutiny.Multi;\n"
"import org.eclipse.microprofile.reactive.messaging.Channel;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:188
#, no-wrap
msgid ""
"import jakarta.inject.Inject;\n"
"import jakarta.ws.rs.GET;\n"
"import jakarta.ws.rs.Path;\n"
"import jakarta.ws.rs.Produces;\n"
"import jakarta.ws.rs.core.MediaType;\n"
"import org.jboss.resteasy.reactive.RestStreamElementType;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:191
#: upstream/_versions/3.2/guides/kafka.adoc:995
#: upstream/_versions/3.2/guides/kafka.adoc:1040
#: upstream/_versions/3.2/guides/kafka.adoc:1076
#, no-wrap
msgid ""
"@Path(\"/prices\")\n"
"public class PriceResource {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:195
#, no-wrap
msgid ""
"    @Inject\n"
"    @Channel(\"prices\")\n"
"    Multi<Double> prices;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:203
#, no-wrap
msgid ""
"    @GET\n"
"    @Path(\"/prices\")\n"
"    @RestStreamElementType(MediaType.TEXT_PLAIN)\n"
"    public Multi<Double> stream() {\n"
"        return prices;\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:207
msgid "This is a good example of how to integrate a Kafka consumer with another downstream, in this example exposing it as a Server-Sent Events endpoint."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:213
msgid "When consuming messages with `@Channel`, the application code is responsible for the subscription.  In the example above, the RESTEasy Reactive endpoint handles that for you."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:216
msgid "Following types can be injected as channels:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:220
#, no-wrap
msgid "@Inject @Channel(\"prices\") Multi<Double> streamOfPayloads;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:222
#, no-wrap
msgid "@Inject @Channel(\"prices\") Multi<Message<Double>> streamOfMessages;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:224
#, no-wrap
msgid "@Inject @Channel(\"prices\") Publisher<Double> publisherOfPayloads;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:226
#, no-wrap
msgid "@Inject @Channel(\"prices\") Publisher<Message<Double>> publisherOfMessages;\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:231
msgid "As with the previous `Message` example, if your injected channel receives payloads (`Multi<T>`), it acknowledges the message automatically, and support multiple subscribers.  If you injected channel receives Message (`Multi<Message<T>>`), you will be responsible for the acknowledgment and broadcasting.  We will explore sending broadcast messages in xref:broadcasting-messages-on-multiple-consumers[Broadcasting messages on multiple consumers]."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:236
msgid "Injecting `@Channel(\"prices\")` or having `@Incoming(\"prices\")` does not automatically configure the application to consume messages from Kafka.  You need to configure an inbound connector with `mp.messaging.incoming.prices\\...` or have an `@Outgoing(\"prices\")` method somewhere in your application (in which case, `prices` will be an in-memory channel)."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:239
#, no-wrap
msgid "Blocking processing"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:245
msgid "Reactive Messaging invokes your method on an I/O thread.  See the xref:quarkus-reactive-architecture.adoc[Quarkus Reactive Architecture documentation] for further details on this topic.  But, you often need to combine Reactive Messaging with blocking processing such as database interactions.  For this, you need to use the `@Blocking` annotation indicating that the processing is _blocking_ and should not be run on the caller thread."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:247
msgid "For example, The following code illustrates how you can store incoming payloads to a database using Hibernate with Panache:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:252
#, no-wrap
msgid ""
"import io.smallrye.reactive.messaging.annotations.Blocking;\n"
"import org.eclipse.microprofile.reactive.messaging.Incoming;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:255
#: upstream/_versions/3.2/guides/kafka.adoc:2436
#, no-wrap
msgid ""
"import jakarta.enterprise.context.ApplicationScoped;\n"
"import jakarta.transaction.Transactional;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:258
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class PriceStorage {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:266
#, no-wrap
msgid ""
"    @Incoming(\"prices\")\n"
"    @Transactional\n"
"    public void store(int priceInUsd) {\n"
"        Price price = new Price();\n"
"        price.value = priceInUsd;\n"
"        price.persist();\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:271
msgid "The complete example is available in the `kafka-panache-quickstart` link:{quickstarts-tree-url}/kafka-panache-quickstart[directory]."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:275
msgid "There are 2 `@Blocking` annotations:"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:277
msgid "`io.smallrye.reactive.messaging.annotations.Blocking`"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:278
msgid "`io.smallrye.common.annotation.Blocking`"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:283
msgid "They have the same effect.  Thus, you can use both.  The first one provides more fine-grained tuning such as the worker pool to use and whether it preserves the order.  The second one, used also with other reactive features of Quarkus, uses the default worker pool and preserves the order."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:285
msgid "Detailed information on the usage of `@Blocking` annotation can be found in https://smallrye.io/smallrye-reactive-messaging/latest/concepts/blocking/[SmallRye Reactive Messaging – Handling blocking execution]."
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:288
#, no-wrap
msgid "@Transactional"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:291
msgid "If your method is annotated with `@Transactional`, it will be considered _blocking_ automatically, even if the method is not annotated with `@Blocking`."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:293
#, no-wrap
msgid "Acknowledgment Strategies"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:301
msgid "All messages received by a consumer must be acknowledged.  In the absence of acknowledgment, the processing is considered in error.  If the consumer method receives a `Record` or a payload, the message will be acked on method return, also known as `Strategy.POST_PROCESSING`.  If the consumer method returns another reactive stream or `CompletionStage`, the message will be acked when the downstream message is acked.  You can override the default behavior to ack the message on arrival (`Strategy.PRE_PROCESSING`), or do not ack the message at all (`Strategy.NONE`) on the consumer method as in the following example:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:309
#, no-wrap
msgid ""
"@Incoming(\"prices\")\n"
"@Acknowledgment(Acknowledgment.Strategy.PRE_PROCESSING)\n"
"public void process(double price) {\n"
"    // process price\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:313
msgid "If the consumer method receives a `Message`, the acknowledgment strategy is `Strategy.MANUAL` and the consumer method is in charge of ack/nack the message."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:321
#, no-wrap
msgid ""
"@Incoming(\"prices\")\n"
"public CompletionStage<Void> process(Message<Double> msg) {\n"
"    // process price\n"
"    return msg.ack();\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:324
msgid "As mentioned above, the method can also override the acknowledgment strategy to `PRE_PROCESSING` or `NONE`."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:326
#, no-wrap
msgid "Commit Strategies"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:332
msgid "When a message produced from a Kafka record is acknowledged, the connector invokes a commit strategy.  These strategies decide when the consumer offset for a specific topic/partition is committed.  Committing an offset indicates that all previous records have been processed.  It is also the position where the application would restart the processing after a crash recovery or a restart."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:335
msgid "Committing every offset has performance penalties as Kafka offset management can be slow.  However, not committing the offset often enough may lead to message duplication if the application crashes between two commits."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:337
msgid "The Kafka connector supports three strategies:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:346
msgid "`throttled` keeps track of received messages and commits an offset of the latest acked message in sequence (meaning, all previous messages were also acked).  This strategy guarantees at-least-once delivery even if the channel performs asynchronous processing.  The connector tracks the received records and periodically (period specified by `auto.commit.interval.ms`, default: 5000 ms) commits the highest consecutive offset.  The connector will be marked as unhealthy if a message associated with a record is not acknowledged in `throttled.unprocessed-record-max-age.ms` (default: 60000 ms).  Indeed, this strategy cannot commit the offset as soon as a single record processing fails.  If `throttled.unprocessed-record-max-age.ms` is set to less than or equal to `0`, it does not perform any health check verification.  Such a setting might lead to running out of memory if there are \"poison pill\" messages (that are never acked).  This strategy is the default if `enable.auto.commit` is not explicitly set to true."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:354
msgid "`checkpoint` allows persisting consumer offsets on a **state store**, instead of committing them back to the Kafka broker.  Using the `CheckpointMetadata` API, consumer code can persist a _processing state_ with the record offset to mark the progress of a consumer.  When the processing continues from a previously persisted offset, it seeks the Kafka consumer to that offset and also restores the persisted state, continuing the stateful processing from where it left off.  The checkpoint strategy holds locally the processing state associated with the latest offset, and persists it periodically to the state store (period specified by `auto.commit.interval.ms` (default: 5000)).  The connector will be marked as unhealthy if no processing state is persisted to the state store in `checkpoint.unsynced-state-max-age.ms` (default: 10000).  If `checkpoint.unsynced-state-max-age.ms` is set to less than or equal to 0, it does not perform any health check verification.  For more information, see xref:stateful-processing-checkpointing[Stateful processing with Checkpointing]"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:358
msgid "`latest` commits the record offset received by the Kafka consumer as soon as the associated message is acknowledged (if the offset is higher than the previously committed offset).  This strategy provides at-least-once delivery if the channel processes the message without performing any asynchronous processing.  This strategy should not be used in high load environment, as offset commit is expensive. However, it reduces the risk of duplicates."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:364
msgid "`ignore` performs no commit. This strategy is the default strategy when the consumer is explicitly configured with `enable.auto.commit` to true.  It delegates the offset commit to the underlying Kafka client.  When `enable.auto.commit` is `true` this strategy **DOES NOT** guarantee at-least-once delivery.  SmallRye Reactive Messaging processes records asynchronously, so offsets may be committed for records that have been polled but not yet processed.  In case of a failure, only records that were not committed yet will be re-processed."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:369
msgid "The Kafka connector disables the Kafka auto commit when it is not explicitly enabled. This behavior differs from the traditional Kafka consumer.  If high throughput is important for you, and you are not limited by the downstream, we recommend to either:"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:371
msgid "use the `throttled` policy,"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:372
msgid "or set `enable.auto.commit` to true and annotate the consuming method with `@Acknowledgment(Acknowledgment.Strategy.NONE)`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:376
msgid "Smallrye Reactive Messaging enables implementing custom commit strategies.  See https://smallrye.io/smallrye-reactive-messaging/latest/kafka/receiving-kafka-records/#acknowledgement[SmallRye Reactive Messaging documentation] for more information."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:378
#, no-wrap
msgid "Error Handling Strategies"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:381
msgid "If a message produced from a Kafka record is nacked, a failure strategy is applied. The Kafka connector supports three strategies:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:383
msgid "`fail`: fail the application, no more records will be processed (default strategy). The offset of the record that has not been processed correctly is not committed."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:384
msgid "`ignore`: the failure is logged, but the processing continue. The offset of the record that has not been processed correctly is committed."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:385
msgid "`dead-letter-queue`: the offset of the record that has not been processed correctly is committed, but the record is written to a Kafka dead letter topic."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:387
msgid "The strategy is selected using the `failure-strategy` attribute."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:389
msgid "In the case of `dead-letter-queue`, you can configure the following attributes:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:391
msgid "`dead-letter-queue.topic`: the topic to use to write the records not processed correctly, default is `dead-letter-topic-$channel`, with `$channel` being the name of the channel."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:392
msgid "`dead-letter-queue.key.serializer`: the serializer used to write the record key on the dead letter queue. By default, it deduces the serializer from the key deserializer."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:393
msgid "`dead-letter-queue.value.serializer`: the serializer used to write the record value on the dead letter queue. By default, it deduces the serializer from the value deserializer."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:395
msgid "The record written on the dead letter queue contains a set of additional headers about the original record:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:397
#, no-wrap
msgid "*dead-letter-reason*: the reason of the failure\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:398
#, no-wrap
msgid "*dead-letter-cause*: the cause of the failure if any\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:399
#, no-wrap
msgid "*dead-letter-topic*: the original topic of the record\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:400
#, no-wrap
msgid "*dead-letter-partition*: the original partition of the record (integer mapped to String)\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:401
#, no-wrap
msgid "*dead-letter-offset*: the original offset of the record (long mapped to String)\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:404
msgid "Smallrye Reactive Messaging enables implementing custom failure strategies.  See https://smallrye.io/smallrye-reactive-messaging/latest/kafka/receiving-kafka-records/#acknowledgement[SmallRye Reactive Messaging documentation] for more information."
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:405
#, no-wrap
msgid "Retrying processing"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:408
msgid "You can combine Reactive Messaging with https://github.com/smallrye/smallrye-fault-tolerance[SmallRye Fault Tolerance], and retry processing if it failed:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:416
#, no-wrap
msgid ""
"@Incoming(\"kafka\")\n"
"@Retry(delay = 10, maxRetries = 5)\n"
"public void consume(String v) {\n"
"   // ... retry if this method throws an exception\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:419
msgid "You can configure the delay, the number of retries, the jitter, etc."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:421
msgid "If your method returns a `Uni` or `CompletionStage`, you need to add the `@NonBlocking` annotation:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:430
#, no-wrap
msgid ""
"@Incoming(\"kafka\")\n"
"@Retry(delay = 10, maxRetries = 5)\n"
"@NonBlocking\n"
"public Uni<String> consume(String v) {\n"
"   // ... retry if this method throws an exception or the returned Uni produce a failure\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:435
msgid "The `@NonBlocking` annotation is only required with SmallRye Fault Tolerance 5.1.0 and earlier.  Starting with SmallRye Fault Tolerance 5.2.0 (available since Quarkus 2.1.0.Final), it is not necessary.  See https://smallrye.io/docs/smallrye-fault-tolerance/5.2.0/usage/extra.html#_non_compatible_mode[SmallRye Fault Tolerance documentation] for more information."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:439
msgid "The incoming messages are acknowledged only once the processing completes successfully.  So, it commits the offset after the successful processing.  If the processing still fails, even after all retries, the message is _nacked_ and the failure strategy is applied."
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:440
#, no-wrap
msgid "Handling Deserialization Failures"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:444
msgid "When a deserialization failure occurs, you can intercept it and provide a failure strategy.  To achieve this, you need to create a bean implementing `DeserializationFailureHandler<T>` interface:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:451
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"@Identifier(\"failure-retry\") // Set the name of the failure handler\n"
"public class MyDeserializationFailureHandler\n"
"    implements DeserializationFailureHandler<JsonObject> { // Specify the expected type\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:460
#, no-wrap
msgid ""
"    @Override\n"
"    public JsonObject decorateDeserialization(Uni<JsonObject> deserialization, String topic, boolean isKey,\n"
"            String deserializer, byte[] data, Headers headers) {\n"
"        return deserialization\n"
"                    .onFailure().retry().atMost(3)\n"
"                    .await().atMost(Duration.ofMillis(200));\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:463
msgid "To use this failure handler, the bean must be exposed with the `@Identifier` qualifier and the connector configuration must specify the attribute `mp.messaging.incoming.$channel.[key|value]-deserialization-failure-handler` (for key or value deserializers)."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:466
msgid "The handler is called with details of the deserialization, including the action represented as `Uni<T>`.  On the deserialization `Uni` failure strategies like retry, providing a fallback value or applying timeout can be implemented."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:467
#, no-wrap
msgid "Consumer Groups"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:474
msgid "In Kafka, a consumer group is a set of consumers which cooperate to consume data from a topic.  A topic is divided into a set of partitions.  The partitions of a topic are assigned among the consumers in the group, effectively allowing to scale consumption throughput.  Note that each partition is assigned to a single consumer from a group.  However, a consumer can be assigned multiple partitions if the number of partitions is greater than the number of consumer in the group."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:476
msgid "Let's explore briefly different producer/consumer patterns and how to implement them using Quarkus:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:478
#, no-wrap
msgid "*Single consumer thread inside a consumer group*\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:482
msgid "This is the default behavior of an application subscribing to a Kafka topic: Each Kafka connector will create a single consumer thread and place it inside a single consumer group.  Consumer group id defaults to the application name as set by the `quarkus.application.name` configuration property.  It can also be set using the `kafka.group.id` property."
msgstr ""

#. type: Named 'alt' AttributeList argument for macro 'image'
#: upstream/_versions/3.2/guides/kafka.adoc:483
#: upstream/_versions/3.2/guides/kafka.adoc:491
#: upstream/_versions/3.2/guides/kafka.adoc:498
#: upstream/_versions/3.2/guides/kafka.adoc:506
#, no-wrap
msgid "Architecture,"
msgstr ""

#. type: Target for macro image
#: upstream/_versions/3.2/guides/kafka.adoc:483
#, no-wrap
msgid "kafka-one-app-one-consumer.png"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:486
#, no-wrap
msgid "*Multiple consumer threads inside a consumer group*\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:490
msgid "For a given application instance, the number of consumers inside the consumer group can be configured using `mp.messaging.incoming.$channel.partitions` property.  The partitions of the subscribed topic will be divided among the consumer threads.  Note that if the `partitions` value exceed the number of partitions of the topic, some consumer threads won't be assigned any partitions."
msgstr ""

#. type: Target for macro image
#: upstream/_versions/3.2/guides/kafka.adoc:491
#, no-wrap
msgid "kafka-one-app-two-consumers.png"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:494
#, no-wrap
msgid "*Multiple consumer applications inside a consumer group*\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:497
msgid "Similar to the previous example, multiple instances of an application can subscribe to a single consumer group, configured via `mp.messaging.incoming.$channel.group.id` property, or left default to the application name.  This in turn will divide partitions of the topic among application instances."
msgstr ""

#. type: Target for macro image
#: upstream/_versions/3.2/guides/kafka.adoc:498
#, no-wrap
msgid "kafka-two-app-one-consumer-group.png"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:501
#, no-wrap
msgid "*Pub/Sub: Multiple consumer groups subscribed to a topic*\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:505
msgid "Lastly different applications can subscribe independently to same topics using different *consumer group ids*.  For example, messages published to a topic called _orders_ can be consumed independently on two consumer applications, one with `mp.messaging.incoming.orders.group.id=invoicing` and second with `mp.messaging.incoming.orders.group.id=shipping`.  Different consumer groups can thus scale independently according to the message consumption requirements."
msgstr ""

#. type: Target for macro image
#: upstream/_versions/3.2/guides/kafka.adoc:506
#, no-wrap
msgid "kafka-two-app-two-consumer-groups.png"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:514
msgid "A common business requirement is to consume and process Kafka records in order.  The Kafka broker preserves order of records inside a partition and not inside a topic.  Therefore, it is important to think about how records are partitioned inside a topic.  The default partitioner uses record key hash to compute the partition for a record, or when the key is not defined, chooses a partition randomly per batch or records."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:517
msgid "During normal operation, a Kafka consumer preserves the order of records inside each partition assigned to it.  Smallrye Reactive Messaging keeps this order for processing, unless `@Blocking(ordered = false)` is used (see xref:blocking-processing[Blocking processing])."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:519
msgid "Note that due to consumer rebalances, Kafka consumers only guarantee at-least-once processing of single records, meaning that uncommitted records _can_ be processed again by consumers."
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:521
#, no-wrap
msgid "Consumer Rebalance Listener"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:528
msgid "Inside a consumer group, as new group members arrive and old members leave, the partitions are re-assigned so that each member receives a proportional share of the partitions.  This is known as rebalancing the group.  To handle offset commit and assigned partitions yourself, you can provide a consumer rebalance listener.  To achieve this, implement the `io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener` interface and expose it as a CDI bean with the `@Idenfier` qualifier.  A common use case is to store offset in a separate data store to implement exactly-once semantic, or starting the processing at a specific offset."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:532
msgid "The listener is invoked every time the consumer topic/partition assignment changes.  For example, when the application starts, it invokes the `partitionsAssigned` callback with the initial set of topics/partitions associated with the consumer.  If, later, this set changes, it calls the `partitionsRevoked` and `partitionsAssigned` callbacks again, so you can implement custom logic."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:535
msgid "Note that the rebalance listener methods are called from the Kafka polling thread and **will** block the caller thread until completion.  That’s because the rebalance protocol has synchronization barriers, and using asynchronous code in a rebalance listener may be executed after the synchronization barrier."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:537
msgid "When topics/partitions are assigned or revoked from a consumer, it pauses the message delivery and resumes once the rebalance completes."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:541
msgid "If the rebalance listener handles offset commit on behalf of the user (using the `NONE` commit strategy), the rebalance listener must commit the offset synchronously in the partitionsRevoked callback.  We also recommend applying the same logic when the application stops."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:543
msgid "Unlike the `ConsumerRebalanceListener` from Apache Kafka, the `io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener` methods pass the Kafka Consumer and the set of topics/partitions."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:547
msgid "In the following example we set up a consumer that always starts on messages from at most 10 minutes ago (or offset 0).  First we need to provide a bean that implements `io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener` and is annotated with `io.smallrye.common.annotation.Identifier`.  We then must configure our inbound connector to use this bean."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:551
#: upstream/_versions/3.2/guides/kafka.adoc:601
#, no-wrap
msgid "package inbound;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:557
#, no-wrap
msgid ""
"import io.smallrye.common.annotation.Identifier;\n"
"import io.smallrye.reactive.messaging.kafka.KafkaConsumerRebalanceListener;\n"
"import org.apache.kafka.clients.consumer.Consumer;\n"
"import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n"
"import org.apache.kafka.clients.consumer.TopicPartition;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:563
#, no-wrap
msgid ""
"import jakarta.enterprise.context.ApplicationScoped;\n"
"import java.util.Collection;\n"
"import java.util.HashMap;\n"
"import java.util.Map;\n"
"import java.util.logging.Logger;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:567
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"@Identifier(\"rebalanced-example.rebalancer\")\n"
"public class KafkaRebalancedConsumerRebalanceListener implements KafkaConsumerRebalanceListener {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:569
#, no-wrap
msgid "    private static final Logger LOGGER = Logger.getLogger(KafkaRebalancedConsumerRebalanceListener.class.getName());\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:581
#, no-wrap
msgid ""
"    /**\n"
"     * When receiving a list of partitions, will search for the earliest offset within 10 minutes\n"
"     * and seek the consumer to it.\n"
"     *\n"
"     * @param consumer   underlying consumer\n"
"     * @param partitions set of assigned topic partitions\n"
"     */\n"
"    @Override\n"
"    public void onPartitionsAssigned(Consumer<?, ?> consumer, Collection<TopicPartition> partitions) {\n"
"        long now = System.currentTimeMillis();\n"
"        long shouldStartAt = now - 600_000L; //10 minute ago\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:594
#, no-wrap
msgid ""
"        Map<TopicPartition, Long> request = new HashMap<>();\n"
"        for (TopicPartition partition : partitions) {\n"
"            LOGGER.info(\"Assigned \" + partition);\n"
"            request.put(partition, shouldStartAt);\n"
"        }\n"
"        Map<TopicPartition, OffsetAndTimestamp> offsets = consumer.offsetsForTimes(request);\n"
"        for (Map.Entry<TopicPartition, OffsetAndTimestamp> position : offsets.entrySet()) {\n"
"            long target = position.getValue() == null ? 0L : position.getValue().offset();\n"
"            LOGGER.info(\"Seeking position \" + target + \" for \" + position.getKey());\n"
"            consumer.seek(position.getKey(), target);\n"
"        }\n"
"    }\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:605
#, no-wrap
msgid ""
"import io.smallrye.reactive.messaging.kafka.IncomingKafkaRecord;\n"
"import org.eclipse.microprofile.reactive.messaging.Acknowledgment;\n"
"import org.eclipse.microprofile.reactive.messaging.Incoming;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:609
#, no-wrap
msgid ""
"import jakarta.enterprise.context.ApplicationScoped;\n"
"import java.util.concurrent.CompletableFuture;\n"
"import java.util.concurrent.CompletionStage;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:612
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class KafkaRebalancedConsumer {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:620
#, no-wrap
msgid ""
"    @Incoming(\"rebalanced-example\")\n"
"    @Acknowledgment(Acknowledgment.Strategy.NONE)\n"
"    public CompletionStage<Void> consume(IncomingKafkaRecord<Integer, String> message) {\n"
"        // We don't need to ACK messages because in this example,\n"
"        // we set offset during consumer rebalance\n"
"        return CompletableFuture.completedFuture(null);\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:626
msgid "To configure the inbound connector to use the provided listener, we either set the consumer rebalance listener’s identifier: `mp.messaging.incoming.rebalanced-example.consumer-rebalance-listener.name=rebalanced-example.rebalancer`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:628
msgid "Or have the listener’s name be the same as the group id:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:630
msgid "`mp.messaging.incoming.rebalanced-example.group.id=rebalanced-example.rebalancer`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:632
msgid "Setting the consumer rebalance listener’s name takes precedence over using the group id."
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:633
#, no-wrap
msgid "Using unique consumer groups"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:636
msgid "If you want to process all the records from a topic (from its beginning), you need:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:638
msgid "to set `auto.offset.reset = earliest`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:639
msgid "assign your consumer to a consumer group not used by any other application."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:642
msgid "Quarkus generates a UUID that changes between two executions (including in dev mode).  So, you are sure no other consumer uses it, and you receive a new unique group id every time your application starts."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:644
msgid "You can use that generated UUID as the consumer group as follows:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:649
#, no-wrap
msgid ""
"mp.messaging.incoming.your-channel.auto.offset.reset=earliest\n"
"mp.messaging.incoming.your-channel.group.id=${quarkus.uuid}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:652
msgid "If the `group.id` attribute is not set, it defaults the `quarkus.application.name` configuration property."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:653
#, no-wrap
msgid "Receiving Kafka Records in Batches"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:657
msgid "By default, incoming methods receive each Kafka record individually.  Under the hood, Kafka consumer clients poll the broker constantly and receive records in batches, presented inside the `ConsumerRecords` container."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:659
msgid "In *batch* mode, your application can receive all the records returned by the consumer *poll* in one go."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:661
msgid "To achieve this you need to specify a compatible container type to receive all the data:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:670
#, no-wrap
msgid ""
"@Incoming(\"prices\")\n"
"public void consume(List<Double> prices) {\n"
"    for (double price : prices) {\n"
"        // process price\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:674
msgid "The incoming method can also receive `Message<List<Payload>>`, `KafkaRecordBatch<Key, Payload>` `ConsumerRecords<Key, Payload>` types.  They give access to record details such as offset or timestamp:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:687
#, no-wrap
msgid ""
"@Incoming(\"prices\")\n"
"public CompletionStage<Void> consumeMessage(KafkaRecordBatch<String, Double> records) {\n"
"    for (KafkaRecord<String, Double> record : records) {\n"
"        String payload = record.getPayload();\n"
"        String topic = record.getTopic();\n"
"        // process messages\n"
"    }\n"
"    // ack will commit the latest offsets (per partition) of the batch.\n"
"    return records.ack();\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:692
msgid "Note that the successful processing of the incoming record batch will commit the latest offsets for each partition received inside the batch.  The configured commit strategy will be applied for these records only."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:694
msgid "Conversely, if the processing throws an exception, all messages are _nacked_, applying the failure strategy for all the records inside the batch."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:699
msgid "Quarkus autodetects batch types for incoming channels and sets batch configuration automatically.  You can configure batch mode explicitly with `mp.messaging.incoming.$channel.batch` property."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:702
#, no-wrap
msgid "Stateful processing with Checkpointing"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:707
msgid "The `checkpoint` commit strategy is an experimental feature and can change in the future."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:714
msgid "Smallrye Reactive Messaging `checkpoint` commit strategy allows consumer applications to process messages in a stateful manner, while also respecting Kafka consumer scalability.  An incoming channel with `checkpoint` commit strategy persists consumer offsets on an external xref:state-stores[state store], such as a relational database or a key-value store.  As a result of processing consumed records, the consumer application can accumulate an internal state for each topic-partition assigned to the Kafka consumer.  This local state will be periodically persisted to the state store and will be associated with the offset of the record that produced it."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:718
msgid "This strategy does not commit any offsets to the Kafka broker, so when new partitions get assigned to the consumer, i.e. consumer restarts or consumer group instances scale, the consumer resumes the processing from the latest _checkpointed_ offset with its saved state."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:721
msgid "The `@Incoming` channel consumer code can manipulate the processing state through the `CheckpointMetadata` API.  For example, a consumer calculating the moving average of prices received on a Kafka topic would look the following:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:725
#: upstream/_versions/3.2/guides/kafka.adoc:825
#: upstream/_versions/3.2/guides/kafka.adoc:2326
#: upstream/_versions/3.2/guides/kafka.adoc:2374
#: upstream/_versions/3.2/guides/kafka.adoc:2415
#: upstream/_versions/3.2/guides/kafka.adoc:2433
#: upstream/_versions/3.2/guides/kafka.adoc:2463
#: upstream/_versions/3.2/guides/kafka.adoc:2495
#: upstream/_versions/3.2/guides/kafka.adoc:2514
#: upstream/_versions/3.2/guides/kafka.adoc:2559
#: upstream/_versions/3.2/guides/kafka.adoc:2602
#: upstream/_versions/3.2/guides/kafka.adoc:2644
#: upstream/_versions/3.2/guides/kafka.adoc:2751
#, no-wrap
msgid "package org.acme;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:727
#: upstream/_versions/3.2/guides/kafka.adoc:2328
#: upstream/_versions/3.2/guides/kafka.adoc:2376
#: upstream/_versions/3.2/guides/kafka.adoc:2604
#, no-wrap
msgid "import java.util.concurrent.CompletionStage;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:732
#, no-wrap
msgid ""
"import org.eclipse.microprofile.reactive.messaging.Incoming;\n"
"import org.eclipse.microprofile.reactive.messaging.Message;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:735
#, no-wrap
msgid ""
"import io.smallrye.reactive.messaging.kafka.KafkaRecord;\n"
"import io.smallrye.reactive.messaging.kafka.commit.CheckpointMetadata;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:738
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class MeanCheckpointConsumer {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:743
#, no-wrap
msgid ""
"    @Incoming(\"prices\")\n"
"    public CompletionStage<Void> consume(Message<Double> record) {\n"
"        // Get the `CheckpointMetadata` from the incoming message\n"
"        CheckpointMetadata<AveragePrice> checkpoint = CheckpointMetadata.fromMessage(record);\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:747
#, no-wrap
msgid ""
"        // `CheckpointMetadata` allows transforming the processing state\n"
"        // Applies the given function, starting from the value `0.0` when no previous state exists\n"
"        checkpoint.transform(new AveragePrice(), average -> average.update(record.getPayload()), /* persistOnAck */ true);\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:752
#, no-wrap
msgid ""
"        // `persistOnAck` flag set to true, ack will persist the processing state\n"
"        // associated with the latest offset (per partition).\n"
"        return record.ack();\n"
"    }\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:756
#, no-wrap
msgid ""
"    static class AveragePrice {\n"
"        long count;\n"
"        double mean;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:763
#, no-wrap
msgid ""
"        AveragePrice update(double newPrice) {\n"
"            mean += ((newPrice - mean) / ++count);\n"
"            return this;\n"
"        }\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:769
msgid "The `transform` method applies the transformation function to the current state, producing a changed state and registering it locally for checkpointing.  By default, the local state is persisted to the state store periodically, period specified by `auto.commit.interval.ms`, (default: 5000).  If `persistOnAck` flag is given, the latest state is persisted to the state store eagerly on message acknowledgment.  The `setNext` method works similarly directly setting the latest state."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:772
msgid "The checkpoint commit strategy tracks when a processing state is last persisted for each topic-partition.  If an outstanding state change can not be persisted for `checkpoint.unsynced-state-max-age.ms` (default: 10000), the channel is marked unhealthy."
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:774
#, no-wrap
msgid "State stores"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:780
msgid "State store implementations determine where and how the processing states are persisted.  This is configured by the `mp.messaging.incoming.[channel-name].checkpoint.state-store` property.  The serialization of state objects depends on the state store implementation.  In order to instruct state stores for serialization can require configuring the class name of state objects using `mp.messaging.incoming.[channel-name].checkpoint.state-type` property."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:782
msgid "Quarkus provides following state store implementations:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:787
msgid "`quarkus-redis`: Uses the xref:redis-reference.adoc[`quarkus-redis-client`] extension to persist processing states.  Jackson is used to serialize processing state in Json. For complex objects it is required to configure the `checkpoint.state-type` property with the class name of the object.  By default, the state store uses the default redis client, but if a xref:redis-reference.adoc#default-and-named-clients[named client] is to be used, the client name can be specified using the `mp.messaging.incoming.[channel-name].checkpoint.quarkus-redis.client-name` property.  Processing states will be stored in Redis using the key naming scheme `[consumer-group-id]:[topic]:[partition]`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:789
#: upstream/_versions/3.2/guides/kafka.adoc:810
msgid "For example the configuration of the previous code would be the following:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:802
#, no-wrap
msgid ""
"mp.messaging.incoming.prices.group.id=prices-checkpoint\n"
"# ...\n"
"mp.messaging.incoming.prices.commit-strategy=checkpoint\n"
"mp.messaging.incoming.prices.checkpoint.state-store=quarkus-redis\n"
"mp.messaging.incoming.prices.checkpoint.state-type=org.acme.MeanCheckpointConsumer.AveragePrice\n"
"# ...\n"
"# if using a named redis client\n"
"mp.messaging.incoming.prices.checkpoint.quarkus-redis.client-name=my-redis\n"
"quarkus.redis.my-redis.hosts=redis://localhost:7000\n"
"quarkus.redis.my-redis.password=<redis-pwd>\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:808
msgid "`quarkus-hibernate-reactive`: Uses the xref:hibernate-reactive.adoc[`quarkus-hibernate-reactive`] extension to persist processing states.  Processing state objects are required to be a Jakarta Persistence entity and extend the `CheckpointEntity` class, which handles object identifiers composed of the consumer group id, topic and partition.  Therefore, the class name of the entity needs to be configured using the `checkpoint.state-type` property."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:818
#, no-wrap
msgid ""
"mp.messaging.incoming.prices.group.id=prices-checkpoint\n"
"# ...\n"
"mp.messaging.incoming.prices.commit-strategy=checkpoint\n"
"mp.messaging.incoming.prices.checkpoint.state-store=quarkus-hibernate-reactive\n"
"mp.messaging.incoming.prices.checkpoint.state-type=org.acme.AveragePriceEntity\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:821
msgid "With `AveragePriceEntity` being a Jakarta Persistence entity extending `CheckpointEntity`:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:827
#: upstream/_versions/3.2/guides/kafka.adoc:2417
#: upstream/_versions/3.2/guides/kafka.adoc:2497
#, no-wrap
msgid "import jakarta.persistence.Entity;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:829
#, no-wrap
msgid "import io.quarkus.smallrye.reactivemessaging.kafka.CheckpointEntity;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:834
#, no-wrap
msgid ""
"@Entity\n"
"public class AveragePriceEntity extends CheckpointEntity {\n"
"    public long count;\n"
"    public double mean;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:840
#, no-wrap
msgid ""
"    public AveragePriceEntity update(double newPrice) {\n"
"        mean += ((newPrice - mean) / ++count);\n"
"        return this;\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:844
msgid "`quarkus-hibernate-orm`: Uses the xref:hibernate-orm.adoc[`quarkus-hibernate-orm`] extension to persist processing states.  It is similar to the previous state store, but it uses Hibernate ORM instead of Hibernate Reactive."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:846
msgid "When configured, it can use a named `persistence-unit` for the checkpointing state store:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:860
#, no-wrap
msgid ""
"mp.messaging.incoming.prices.commit-strategy=checkpoint\n"
"mp.messaging.incoming.prices.checkpoint.state-store=quarkus-hibernate-orm\n"
"mp.messaging.incoming.prices.checkpoint.state-type=org.acme.AveragePriceEntity\n"
"mp.messaging.incoming.prices.checkpoint.quarkus-hibernate-orm.persistence-unit=prices\n"
"# ... Setup \"prices\" persistence unit\n"
"quarkus.datasource.\"prices\".db-kind=postgresql\n"
"quarkus.datasource.\"prices\".username=<your username>\n"
"quarkus.datasource.\"prices\".password=<your password>\n"
"quarkus.datasource.\"prices\".jdbc.url=jdbc:postgresql://localhost:5432/hibernate_orm_test\n"
"quarkus.hibernate-orm.\"prices\".datasource=prices\n"
"quarkus.hibernate-orm.\"prices\".packages=org.acme\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:864
msgid "For instructions on how to implement custom state stores, see https://smallrye.io/smallrye-reactive-messaging/3.22.0/kafka/receiving-kafka-records/#implementing-state-stores[Implementing State Stores]."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:865
#, no-wrap
msgid "Sending messages to Kafka"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:868
msgid "Configuration for the Kafka connector outgoing channels is similar to that of incoming:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:874
#, no-wrap
msgid ""
"%prod.kafka.bootstrap.servers=kafka:9092 <1>\n"
"mp.messaging.outgoing.prices-out.connector=smallrye-kafka <2>\n"
"mp.messaging.outgoing.prices-out.topic=prices <3>\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:879
msgid "Configure the broker location for the production profile. You can configure it globally or per channel using `mp.messaging.outgoing.$channel.bootstrap.servers` property.  In dev mode and when running tests, xref:kafka-dev-services[Dev Services for Kafka] automatically starts a Kafka broker.  When not provided, this property defaults to `localhost:9092`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:880
msgid "Configure the connector to manage the `prices-out` channel."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:881
msgid "By default, the topic name is same as the channel name. You can configure the topic attribute to override it."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:886
msgid "Inside application configuration, channel names are unique.  Therefore, if you'd like to configure an incoming and outgoing channel on the same topic, you will need to name channels differently (like in the examples of this guide, `mp.messaging.incoming.prices` and `mp.messaging.outgoing.prices-out`)."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:890
msgid "Then, your application can generate messages and publish them to the `prices-out` channel.  It can use `double` payloads as in the following snippet:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:895
#, no-wrap
msgid ""
"import io.smallrye.mutiny.Multi;\n"
"import org.eclipse.microprofile.reactive.messaging.Outgoing;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:899
#, no-wrap
msgid ""
"import jakarta.enterprise.context.ApplicationScoped;\n"
"import java.time.Duration;\n"
"import java.util.Random;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:902
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class KafkaPriceProducer {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:904
#: upstream/_versions/3.2/guides/kafka.adoc:1216
#, no-wrap
msgid "    private final Random random = new Random();\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:912
#, no-wrap
msgid ""
"    @Outgoing(\"prices-out\")\n"
"    public Multi<Double> generate() {\n"
"        // Build an infinite stream of random prices\n"
"        // It emits a price every second\n"
"        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n"
"            .map(x -> random.nextDouble());\n"
"    }\n"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:919
msgid "You should not call methods annotated with `@Incoming` and/or `@Outgoing` directly from your code. They are invoked by the framework. Having user code invoking them would not have the expected outcome."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:923
msgid "Note that the `generate` method returns a `Multi<Double>`, which implements the Reactive Streams `Publisher` interface.  This publisher will be used by the framework to generate messages and send them to the configured Kafka topic."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:925
msgid "Instead of returning a payload, you can return a `io.smallrye.reactive.messaging.kafka.Record` to send key/value pairs:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:933
#, no-wrap
msgid ""
"@Outgoing(\"out\")\n"
"public Multi<Record<String, Double>> generate() {\n"
"    return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n"
"        .map(x -> Record.of(\"my-key\", random.nextDouble()));\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:936
msgid "Payload can be wrapped inside `org.eclipse.microprofile.reactive.messaging.Message` to have more control on the written records:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:949
#, no-wrap
msgid ""
"@Outgoing(\"generated-price\")\n"
"public Multi<Message<Double>> generate() {\n"
"    return Multi.createFrom().ticks().every(Duration.ofSeconds(1))\n"
"            .map(x -> Message.of(random.nextDouble())\n"
"                    .addMetadata(OutgoingKafkaRecordMetadata.<String>builder()\n"
"                            .withKey(\"my-key\")\n"
"                            .withTopic(\"my-key-prices\")\n"
"                            .withHeaders(new RecordHeaders().add(\"my-header\", \"value\".getBytes()))\n"
"                            .build()));\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:954
msgid "`OutgoingKafkaRecordMetadata` allows to set metadata attributes of the Kafka record, such as `key`, `topic`, `partition` or `timestamp`.  One use case is to dynamically select the destination topic of a message.  In this case, instead of configuring the topic inside your application configuration file, you need to use the outgoing metadata to set the name of the topic."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:957
msgid "Other than method signatures returning a Reactive Stream `Publisher` (`Multi` being an implementation of `Publisher`), outgoing method can also return single message.  In this case the producer will use this method as generator to create an infinite stream."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:961
#, no-wrap
msgid "@Outgoing(\"prices-out\") T generate(); // T excluding void\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:963
#, no-wrap
msgid "@Outgoing(\"prices-out\") Message<T> generate();\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:965
#, no-wrap
msgid "@Outgoing(\"prices-out\") Uni<T> generate();\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:967
#, no-wrap
msgid "@Outgoing(\"prices-out\") Uni<Message<T>> generate();\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:969
#, no-wrap
msgid "@Outgoing(\"prices-out\") CompletionStage<T> generate();\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:971
#, no-wrap
msgid "@Outgoing(\"prices-out\") CompletionStage<Message<T>> generate();\n"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:973
#, no-wrap
msgid "Sending messages with @Emitter"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:976
msgid "Sometimes, you need to have an imperative way of sending messages."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:979
msgid "For example, if you need to send a message to a stream when receiving a POST request inside a REST endpoint.  In this case, you cannot use `@Outgoing` because your method has parameters."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:981
msgid "For this, you can use an `Emitter`."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:986
#: upstream/_versions/3.2/guides/kafka.adoc:2336
#: upstream/_versions/3.2/guides/kafka.adoc:2384
#: upstream/_versions/3.2/guides/kafka.adoc:2611
#, no-wrap
msgid ""
"import org.eclipse.microprofile.reactive.messaging.Channel;\n"
"import org.eclipse.microprofile.reactive.messaging.Emitter;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:992
#: upstream/_versions/3.2/guides/kafka.adoc:1037
#: upstream/_versions/3.2/guides/kafka.adoc:1071
#, no-wrap
msgid ""
"import jakarta.inject.Inject;\n"
"import jakarta.ws.rs.POST;\n"
"import jakarta.ws.rs.Path;\n"
"import jakarta.ws.rs.Consumes;\n"
"import jakarta.ws.rs.core.MediaType;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:999
#, no-wrap
msgid ""
"    @Inject\n"
"    @Channel(\"price-create\")\n"
"    Emitter<Double> priceEmitter;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1006
#, no-wrap
msgid ""
"    @POST\n"
"    @Consumes(MediaType.TEXT_PLAIN)\n"
"    public void addPrice(Double price) {\n"
"        CompletionStage<Void> ack = priceEmitter.send(price);\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1009
msgid "Sending a payload returns a `CompletionStage`, completed when the message is acked. If the message transmission fails, the `CompletionStage` is completed exceptionally with the reason of the nack."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1013
msgid "The `Emitter` configuration is done the same way as the other stream configuration used by `@Incoming` and `@Outgoing`."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1022
msgid "Using the `Emitter` you are sending messages from your imperative code to reactive messaging.  These messages are stored in a queue until they are sent.  If the Kafka producer client can't keep up with messages trying to be sent over to Kafka, this queue can become a memory hog and you may even run out of memory.  You can use `@OnOverflow` to configure back-pressure strategy.  It lets you configure the size of the queue (default is 256) and the strategy to apply when the buffer size is reached. Available strategies are `DROP`, `LATEST`, `FAIL`, `BUFFER`, `UNBOUNDED_BUFFER` and `NONE`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1025
msgid "With the `Emitter` API, you can also encapsulate the outgoing payload inside `Message<T>`. As with the previous examples, `Message` lets you handle the ack/nack cases differently."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1031
#, no-wrap
msgid ""
"import java.util.concurrent.CompletableFuture;\n"
"import org.eclipse.microprofile.reactive.messaging.Channel;\n"
"import org.eclipse.microprofile.reactive.messaging.Emitter;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1042
#, no-wrap
msgid "    @Inject @Channel(\"price-create\") Emitter<Double> priceEmitter;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1057
#, no-wrap
msgid ""
"    @POST\n"
"    @Consumes(MediaType.TEXT_PLAIN)\n"
"    public void addPrice(Double price) {\n"
"        priceEmitter.send(Message.of(price)\n"
"            .withAck(() -> {\n"
"                // Called when the message is acked\n"
"                return CompletableFuture.completedFuture(null);\n"
"            })\n"
"            .withNack(throwable -> {\n"
"                // Called when the message is nacked\n"
"                return CompletableFuture.completedFuture(null);\n"
"            }));\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1061
msgid "If you prefer using Reactive Stream APIs, you can use `MutinyEmitter` that will return `Uni<Void>` from the `send` method.  You can therefore use Mutiny APIs for handling downstream messages and errors."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1065
#: upstream/_versions/3.2/guides/kafka.adoc:1253
#: upstream/_versions/3.2/guides/kafka.adoc:2649
#, no-wrap
msgid "import org.eclipse.microprofile.reactive.messaging.Channel;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1073
#, no-wrap
msgid "import io.smallrye.reactive.messaging.MutinyEmitter;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1080
#, no-wrap
msgid ""
"    @Inject\n"
"    @Channel(\"price-create\")\n"
"    MutinyEmitter<Double> priceEmitter;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1089
#, no-wrap
msgid ""
"    @POST\n"
"    @Consumes(MediaType.TEXT_PLAIN)\n"
"    public Uni<String> addPrice(Double price) {\n"
"        return quoteRequestEmitter.send(price)\n"
"                .map(x -> \"ok\")\n"
"                .onFailure().recoverWithItem(\"ko\");\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1093
msgid "It is also possible to block on sending the event to the emitter with the `sendAndAwait` method.  It will only return from the method when the event is acked or nacked by the receiver."
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:1095
#, no-wrap
msgid "Deprecation"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1098
msgid "The `io.smallrye.reactive.messaging.annotations.Emitter`, `io.smallrye.reactive.messaging.annotations.Channel` and `io.smallrye.reactive.messaging.annotations.OnOverflow` classes are now deprecated and replaced by:"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1100
msgid "`org.eclipse.microprofile.reactive.messaging.Emitter`"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1101
msgid "`org.eclipse.microprofile.reactive.messaging.Channel`"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1102
msgid "`org.eclipse.microprofile.reactive.messaging.OnOverflow`"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1104
msgid "The new `Emitter.send` method returns a `CompletionStage` completed when the produced message is acknowledged."
msgstr ""

#. type: Block title
#: upstream/_versions/3.2/guides/kafka.adoc:1107
#, no-wrap
msgid "Depreciation"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1110
msgid "`MutinyEmitter#send(Message msg)` method is deprecated in favor of following methods receiving `Message` for emitting:"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1112
msgid "`<M extends Message<? extends T>> Uni<Void> sendMessage(M msg)`"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1113
msgid "`<M extends Message<? extends T>> void sendMessageAndAwait(M msg)`"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1114
msgid "`<M extends Message<? extends T>> Cancellable sendMessageAndForget(M msg)`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1118
msgid "More information on how to use `Emitter` can be found in https://smallrye.io/smallrye-reactive-messaging/latest/concepts/emitter/[SmallRye Reactive Messaging – Emitters and Channels]"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1119
#, no-wrap
msgid "Write Acknowledgement"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1123
msgid "When Kafka broker receives a record, its acknowledgement can take time depending on the configuration.  Also, it stores in-memory the records that cannot be written."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1126
msgid "By default, the connector does wait for Kafka to acknowledge the record to continue the processing (acknowledging the received Message).  You can disable this by setting the `waitForWriteCompletion` attribute to `false`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1128
msgid "Note that the `acks` attribute has a huge impact on the record acknowledgement."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1130
msgid "If a record cannot be written, the message is nacked."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1131
#, no-wrap
msgid "Backpressure"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1135
msgid "The Kafka outbound connector handles back-pressure, monitoring the number of in-flight messages waiting to be written to the Kafka broker.  The number of in-flight messages is configured using the `max-inflight-messages` attribute and defaults to 1024."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1140
msgid "The connector only sends that amount of messages concurrently.  No other messages will be sent until at least one in-flight message gets acknowledged by the broker.  Then, the connector writes a new message to Kafka when one of the broker’s in-flight messages get acknowledged.  Be sure to configure Kafka’s `batch.size` and `linger.ms` accordingly."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1143
msgid "You can also remove the limit of in-flight messages by setting `max-inflight-messages` to `0`.  However, note that the Kafka producer may block if the number of requests reaches `max.in.flight.requests.per.connection`."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1144
#, no-wrap
msgid "Retrying message dispatch"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1149
msgid "When the Kafka producer receives an error from the server, if it is a transient, recoverable error, the client will retry sending the batch of messages.  This behavior is controlled by `retries` and `retry.backoff.ms` parameters.  In addition to this, SmallRye Reactive Messaging will retry individual messages on recoverable errors, depending on the `retries` and `delivery.timeout.ms` parameters."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1152
msgid "Note that while having retries in a reliable system is a best practice, the `max.in.flight.requests.per.connection` parameter defaults to `5`, meaning that the order of the messages is not guaranteed.  If the message order is a must for your use case, setting `max.in.flight.requests.per.connection` to `1` will make sure a single batch of messages is sent at a time, in the expense of limiting the throughput of the producer."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1154
msgid "For applying retry mechanism on processing errors, see the section on xref:retrying-processing[Retrying processing]."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1155
#, no-wrap
msgid "Handling Serialization Failures"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1159
msgid "For Kafka producer client serialization failures are not recoverable, thus the message dispatch is not retried. In these cases you may need to apply a failure strategy for the serializer.  To achieve this, you need to create a bean implementing `SerializationFailureHandler<T>` interface:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1166
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"@Identifier(\"failure-fallback\") // Set the name of the failure handler\n"
"public class MySerializationFailureHandler\n"
"    implements SerializationFailureHandler<JsonObject> { // Specify the expected type\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1175
#, no-wrap
msgid ""
"    @Override\n"
"    public byte[] decorateSerialization(Uni<byte[]> serialization, String topic, boolean isKey,\n"
"        String serializer, Object data, Headers headers) {\n"
"        return serialization\n"
"                    .onFailure().retry().atMost(3)\n"
"                    .await().indefinitely();\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1178
msgid "To use this failure handler, the bean must be exposed with the `@Identifier` qualifier and the connector configuration must specify the attribute `mp.messaging.outgoing.$channel.[key|value]-serialization-failure-handler` (for key or value serializers)."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1181
msgid "The handler is called with details of the serialization, including the action represented as `Uni<byte[]>`.  Note that the method must await on the result and return the serialized byte array."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1182
#, no-wrap
msgid "In-memory channels"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1187
msgid "In some use cases, it is convenient to use the messaging patterns to transfer messages inside the same application.  When you don't connect a channel to a messaging backend like Kafka, everything happens in-memory, and the streams are created by chaining methods together.  Each chain is still a reactive stream and enforces the back-pressure protocol."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1191
msgid "The framework verifies that the producer/consumer chain is complete, meaning that if the application writes messages into an in-memory channel (using a method with only `@Outgoing`, or an `Emitter`), it must also consume the messages from within the application (using a method with only `@Incoming` or using an unmanaged stream)."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1193
#, no-wrap
msgid "Broadcasting messages on multiple consumers"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1198
msgid "By default, a channel can be linked to a single consumer, using `@Incoming` method or `@Channel` reactive stream.  At application startup, channels are verified to form a chain of consumers and producers with single consumer and producer.  You can override this behavior by setting `mp.messaging.$channel.broadcast=true` on a channel."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1200
msgid "In case of in-memory channels, `@Broadcast` annotation can be used on the `@Outgoing` method. For example,"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1204
#, no-wrap
msgid "import java.util.Random;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1209
#: upstream/_versions/3.2/guides/kafka.adoc:1320
#: upstream/_versions/3.2/guides/kafka.adoc:1348
#, no-wrap
msgid ""
"import org.eclipse.microprofile.reactive.messaging.Incoming;\n"
"import org.eclipse.microprofile.reactive.messaging.Outgoing;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1211
#, no-wrap
msgid "import io.smallrye.reactive.messaging.annotations.Broadcast;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1214
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class MultipleConsumer {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1222
#, no-wrap
msgid ""
"    @Outgoing(\"in-memory-channel\")\n"
"    @Broadcast\n"
"    double generate() {\n"
"        return random.nextDouble();\n"
"    }\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1227
#, no-wrap
msgid ""
"    @Incoming(\"in-memory-channel\")\n"
"    void consumeAndLog(double price) {\n"
"        System.out.println(price);\n"
"    }\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1234
#, no-wrap
msgid ""
"    @Incoming(\"in-memory-channel\")\n"
"    @Outgoing(\"prices2\")\n"
"    double consumeAndSend(double price) {\n"
"        return price;\n"
"    }\n"
"}\n"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1240
msgid "Reciprocally, multiple producers on the same channel can be merged by setting `mp.messaging.incoming.$channel.merge=true`.  On the `@Incoming` methods, you can control how multiple channels are merged using the `@Merge` annotation."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1242
#, no-wrap
msgid "Kafka Transactions"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1247
msgid "Kafka transactions enable atomic writes to multiple Kafka topics and partitions.  The Kafka connector provides `KafkaTransactions` custom emitter for writing Kafka records inside a transaction.  It can be injected as a regular emitter `@Channel`:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1257
#, no-wrap
msgid ""
"import io.smallrye.mutiny.Uni;\n"
"import io.smallrye.reactive.messaging.kafka.KafkaRecord;\n"
"import io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1260
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class KafkaTransactionalProducer {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1263
#, no-wrap
msgid ""
"    @Channel(\"tx-out-example\")\n"
"    KafkaTransactions<String> txProducer;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1272
#, no-wrap
msgid ""
"    public Uni<Void> emitInTransaction() {\n"
"        return txProducer.withTransaction(emitter -> {\n"
"            emitter.send(KafkaRecord.of(1, \"a\"));\n"
"            emitter.send(KafkaRecord.of(2, \"b\"));\n"
"            emitter.send(KafkaRecord.of(3, \"c\"));\n"
"            return Uni.createFrom().voidItem();\n"
"        });\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1277
msgid "The function given to the `withTransaction` method receives a `TransactionalEmitter` for producing records, and returns a `Uni` that provides the result of the transaction."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1279
msgid "If the processing completes successfully, the producer is flushed and the transaction is committed."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1280
msgid "If the processing throws an exception, returns a failing `Uni`, or marks the `TransactionalEmitter` for abort, the transaction is aborted."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1284
msgid "Kafka transactional producers require configuring `acks=all` client property, and a unique id for `transactional.id`, which implies `enable.idempotence=true`.  When Quarkus detects the use of `KafkaTransactions` for an outgoing channel it configures these properties on the channel, providing a default value of `\"${quarkus.application.name}-${channelName}\"` for `transactional.id` property."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1286
msgid "Note that for production use the `transactional.id` must be unique across all application instances."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1294
msgid "While a normal message emitter would support concurrent calls to `send` methods and consequently queues outgoing messages to be written to Kafka, a `KafkaTransactions` emitter only supports one transaction at a time.  A transaction is considered in progress from the call to the `withTransaction` until the returned `Uni` results in success or failure.  While a transaction is in progress, subsequent calls to the `withTransaction`, including nested ones inside the given function, will throw `IllegalStateException`."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1298
msgid "Note that in Reactive Messaging, the execution of processing methods, is already serialized, unless `@Blocking(ordered = false)` is used.  If `withTransaction` can be called concurrently, for example from a REST endpoint, it is recommended to limit the concurrency of the execution.  This can be done using the `@Bulkhead` annotation from xref:smallrye-fault-tolerance.adoc[_Microprofile Fault Tolerance_]."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1300
msgid "An example usage can be found in xref:chaining-kafka-transactions-with-hibernate-reactive-transactions[Chaining Kafka Transactions with Hibernate Reactive transactions]."
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:1302
#, no-wrap
msgid "Transaction-aware consumers"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1305
msgid "If you'd like to consume records only written and committed inside a Kafka transaction you need to configure the `isolation.level` property on the incoming channel as such:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1309
#, no-wrap
msgid "mp.messaging.incoming.prices-in.isolation.level=read_committed\n"
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1311
#, no-wrap
msgid "Processing Messages"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1315
msgid "Applications streaming data often need to consume some events from a topic, process them and publish the result to a different topic.  A processor method can be simply implemented using both the `@Incoming` and `@Outgoing` annotations:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1325
#: upstream/_versions/3.2/guides/kafka.adoc:1353
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class PriceProcessor {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1327
#: upstream/_versions/3.2/guides/kafka.adoc:1355
#: upstream/_versions/3.2/guides/kafka.adoc:1566
#, no-wrap
msgid "    private static final double CONVERSION_RATE = 0.88;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1333
#, no-wrap
msgid ""
"    @Incoming(\"price-in\")\n"
"    @Outgoing(\"price-out\")\n"
"    public double process(double price) {\n"
"        return price * CONVERSION_RATE;\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1339
msgid "The parameter of the `process` method is the incoming message payload, whereas the return value will be used as the outgoing message payload.  Previously mentioned signatures for parameter and return types are also supported, such as `Message<T>`, `Record<K, V>`, etc."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1341
msgid "You can apply asynchronous stream processing by consuming and returning reactive stream `Multi<T>` type:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1350
#, no-wrap
msgid "import io.smallrye.mutiny.Multi;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1361
#, no-wrap
msgid ""
"    @Incoming(\"price-in\")\n"
"    @Outgoing(\"price-out\")\n"
"    public Multi<Double> process(Multi<Integer> prices) {\n"
"        return prices.filter(p -> p > 100).map(p -> p * CONVERSION_RATE);\n"
"    }\n"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1365
#, no-wrap
msgid "Propagating Record Key"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1368
msgid "When processing messages, you can propagate incoming record key to the outgoing record."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1371
msgid "Enabled with `mp.messaging.outgoing.$channel.propagate-record-key=true` configuration, record key propagation produces the outgoing record with the same _key_ as the incoming record."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1374
msgid "If the outgoing record already contains a _key_, it *won't be overridden* by the incoming record key.  If the incoming record does have a _null_ key, the `mp.messaging.outgoing.$channel.key` property is used."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1375
#, no-wrap
msgid "Exactly-Once Processing"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1379
msgid "Kafka Transactions allows managing consumer offsets inside a transaction, together with produced messages.  This enables coupling a consumer with a transactional producer in a _consume-transform-produce_ pattern, also known as *exactly-once processing*."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1381
msgid "The `KafkaTransactions` custom emitter provides a way to apply exactly-once processing to an incoming Kafka message inside a transaction."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1383
msgid "The following example includes a batch of Kafka records inside a transaction."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1391
#, no-wrap
msgid ""
"import org.eclipse.microprofile.reactive.messaging.Channel;\n"
"import org.eclipse.microprofile.reactive.messaging.Incoming;\n"
"import org.eclipse.microprofile.reactive.messaging.OnOverflow;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1396
#, no-wrap
msgid ""
"import io.smallrye.mutiny.Uni;\n"
"import io.smallrye.reactive.messaging.kafka.KafkaRecord;\n"
"import io.smallrye.reactive.messaging.kafka.KafkaRecordBatch;\n"
"import io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1399
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class KafkaExactlyOnceProcessor {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1403
#, no-wrap
msgid ""
"    @Channel(\"prices-out\")\n"
"    @OnOverflow(value = OnOverflow.Strategy.BUFFER, bufferSize = 500) // <3>\n"
"    KafkaTransactions<Integer> txProducer;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1413
#, no-wrap
msgid ""
"    @Incoming(\"prices-in\")\n"
"    public Uni<Void> emitInTransaction(KafkaRecordBatch<String, Integer> batch) { // <1>\n"
"        return txProducer.withTransactionAndAck(batch, emitter -> { // <2>\n"
"            for (KafkaRecord<String, Integer> record : batch) {\n"
"                emitter.send(KafkaRecord.of(record.getKey(), record.getPayload() + 1)); // <3>\n"
"            }\n"
"            return Uni.createFrom().voidItem();\n"
"        });\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1419
msgid "It is recommended to use exactly-once processing along with the batch consumption mode.  While it is possible to use it with a single Kafka message, it'll have a significant performance impact."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1420
msgid "The consumed `KafkaRecordBatch` message is passed to the `KafkaTransactions#withTransactionAndAck` in order to handle the offset commits and message acks."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1423
msgid "The `send` method writes records to Kafka inside the transaction, without waiting for send receipt from the broker.  Messages pending to be written to Kafka will be buffered, and flushed before committing the transaction.  It is therefore recommended configuring the `@OnOverflow` `bufferSize` in order to fit enough messages, for example the `max.poll.records`, maximum amount of records returned in a batch."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1425
msgid "If the processing completes successfully, _before committing the transaction_, the topic partition offsets of the given batch message will be committed to the transaction."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1426
msgid "If the processing needs to abort, _after aborting the transaction_, the consumer's position is reset to the last committed offset, effectively resuming the consumption from that offset. If no consumer offset has been committed to a topic-partition, the consumer's position is reset to the beginning of the topic-partition, _even if the offset reset policy is `latest`_."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1429
msgid "When using exactly-once processing, consumed message offset commits are handled by the transaction and therefore the application should not commit offsets through other means.  The consumer should have `enable.auto.commit=false` (the default) and set explicitly `commit-strategy=ignore`:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1434
#, no-wrap
msgid ""
"mp.messaging.incoming.prices-in.commit-strategy=ignore\n"
"mp.messaging.incoming.prices-in.failure-strategy=ignore\n"
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:1436
#, no-wrap
msgid "Error handling for the exactly-once processing"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1440
msgid "The `Uni` returned from the `KafkaTransactions#withTransaction` will yield a failure if the transaction fails and is aborted.  The application can choose to handle the error case, but if a failing `Uni` is returned from the `@Incoming` method, the incoming channel will effectively fail and stop the reactive stream."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1444
msgid "The `KafkaTransactions#withTransactionAndAck` method acks and nacks the message but will *not* return a failing `Uni`.  Nacked messages will be handled by the failure strategy of the incoming channel, (see xref:error-handling[Error Handling Strategies]).  Configuring `failure-strategy=ignore` simply resets the Kafka consumer to the last committed offsets and resumes the consumption from there."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1447
#, no-wrap
msgid "Accessing Kafka clients directly"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1451
msgid "In rare cases, you may need to access the underlying Kafka clients.  `KafkaClientService` provides thread-safe access to `Producer` and `Consumer`."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1457
#, no-wrap
msgid ""
"import jakarta.enterprise.context.ApplicationScoped;\n"
"import jakarta.enterprise.event.Observes;\n"
"import jakarta.inject.Inject;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1459
#, no-wrap
msgid "import org.apache.kafka.clients.producer.ProducerRecord;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1464
#, no-wrap
msgid ""
"import io.quarkus.runtime.StartupEvent;\n"
"import io.smallrye.reactive.messaging.kafka.KafkaClientService;\n"
"import io.smallrye.reactive.messaging.kafka.KafkaConsumer;\n"
"import io.smallrye.reactive.messaging.kafka.KafkaProducer;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1467
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class PriceSender {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1470
#, no-wrap
msgid ""
"    @Inject\n"
"    KafkaClientService clientService;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1477
#, no-wrap
msgid ""
"    void onStartup(@Observes StartupEvent startupEvent) {\n"
"        KafkaProducer<String, Double> producer = clientService.getProducer(\"generated-price\");\n"
"        producer.runOnSendingThread(client -> client.send(new ProducerRecord<>(\"prices\", 2.4)))\n"
"            .await().indefinitely();\n"
"    }\n"
"}\n"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:1482
msgid "The `KafkaClientService` is an experimental API and can change in the future."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1485
msgid "You can also get the Kafka configuration injected to your application and create Kafka producer, consumer and admin clients directly:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1492
#, no-wrap
msgid ""
"import io.smallrye.common.annotation.Identifier;\n"
"import org.apache.kafka.clients.admin.AdminClient;\n"
"import org.apache.kafka.clients.admin.AdminClientConfig;\n"
"import org.apache.kafka.clients.admin.KafkaAdminClient;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1498
#, no-wrap
msgid ""
"import jakarta.enterprise.context.ApplicationScoped;\n"
"import jakarta.enterprise.inject.Produces;\n"
"import jakarta.inject.Inject;\n"
"import java.util.HashMap;\n"
"import java.util.Map;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1501
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class KafkaClients {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1505
#, no-wrap
msgid ""
"    @Inject\n"
"    @Identifier(\"default-kafka-broker\")\n"
"    Map<String, Object> config;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1516
#, no-wrap
msgid ""
"    @Produces\n"
"    AdminClient getAdmin() {\n"
"        Map<String, Object> copy = new HashMap<>();\n"
"        for (Map.Entry<String, Object> entry : config.entrySet()) {\n"
"            if (AdminClientConfig.configNames().contains(entry.getKey())) {\n"
"                copy.put(entry.getKey(), entry.getValue());\n"
"            }\n"
"        }\n"
"        return KafkaAdminClient.create(copy);\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1523
msgid "The `default-kafka-broker` configuration map contains all application properties prefixed with `kafka.` or `KAFKA_`.  For more configuration options check out xref:kafka-configuration-resolution[Kafka Configuration Resolution]."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1525
#, no-wrap
msgid "JSON serialization"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1528
msgid "Quarkus has built-in capabilities to deal with JSON Kafka messages."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1530
msgid "Imagine we have a `Fruit` data class as follows:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1534
#, no-wrap
msgid "public class Fruit {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1537
#, no-wrap
msgid ""
"    public String name;\n"
"    public int price;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1540
#, no-wrap
msgid ""
"    public Fruit() {\n"
"    }\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1546
#, no-wrap
msgid ""
"    public Fruit(String name, int price) {\n"
"        this.name = name;\n"
"        this.price = price;\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1549
msgid "And we want to use it to receive messages from Kafka, make some price transformation, and send messages back to Kafka."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1555
#, no-wrap
msgid ""
"import io.smallrye.reactive.messaging.annotations.Broadcast;\n"
"import org.eclipse.microprofile.reactive.messaging.Incoming;\n"
"import org.eclipse.microprofile.reactive.messaging.Outgoing;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1564
#, no-wrap
msgid ""
"/**\n"
"* A bean consuming data from the \"fruit-in\" channel and applying some price conversion.\n"
"* The result is pushed to the \"fruit-out\" channel.\n"
"*/\n"
"@ApplicationScoped\n"
"public class FruitProcessor {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1574
#, no-wrap
msgid ""
"    @Incoming(\"fruit-in\")\n"
"    @Outgoing(\"fruit-out\")\n"
"    @Broadcast\n"
"    public Fruit process(Fruit fruit) {\n"
"        fruit.price = fruit.price * CONVERSION_RATE;\n"
"        return fruit;\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1579
msgid "To do this, we will need to set up JSON serialization with Jackson or JSON-B."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1581
msgid "With JSON serialization correctly configured, you can also use `Publisher<Fruit>` and `Emitter<Fruit>`."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1583
#, no-wrap
msgid "Serializing via Jackson"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1588
msgid "Quarkus has built-in support for JSON serialization and deserialization based on Jackson.  It will also xref:serialization-generation[generate] the serializer and deserializer for you, so you do not have to configure anything.  When generation is disabled, you can use the provided `ObjectMapperSerializer` and `ObjectMapperDeserializer` as explained below."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1591
msgid "There is an existing `ObjectMapperSerializer` that can be used to serialize all data objects via Jackson.  You may create an empty subclass if you want to use xref:serialization-autodetection[Serializer/deserializer autodetection]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1595
msgid "By default, the `ObjectMapperSerializer` serializes null as the `\"null\"` String, this can be customized by setting the Kafka configuration property `json.serialize.null-as-null=true` which will serialize null as `null`.  This is handy when using a compacted topic, as `null` is used as a tombstone to know which messages delete during compaction phase."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1598
msgid "The corresponding deserializer class needs to be subclassed.  So, let's create a `FruitDeserializer` that extends the `ObjectMapperDeserializer`."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1602
#: upstream/_versions/3.2/guides/kafka.adoc:1633
#, no-wrap
msgid "package com.acme.fruit.jackson;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1604
#: upstream/_versions/3.2/guides/kafka.adoc:2465
#: upstream/_versions/3.2/guides/kafka.adoc:2561
#, no-wrap
msgid "import io.quarkus.kafka.client.serialization.ObjectMapperDeserializer;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1610
#: upstream/_versions/3.2/guides/kafka.adoc:2471
#: upstream/_versions/3.2/guides/kafka.adoc:2567
#, no-wrap
msgid ""
"public class FruitDeserializer extends ObjectMapperDeserializer<Fruit> {\n"
"    public FruitDeserializer() {\n"
"        super(Fruit.class);\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1613
msgid "Finally, configure your channels to use the Jackson serializer and deserializer."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1619
#, no-wrap
msgid ""
"# Configure the Kafka source (we read from it)\n"
"mp.messaging.incoming.fruit-in.topic=fruit-in\n"
"mp.messaging.incoming.fruit-in.value.deserializer=com.acme.fruit.jackson.FruitDeserializer\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1623
#, no-wrap
msgid ""
"# Configure the Kafka sink (we write to it)\n"
"mp.messaging.outgoing.fruit-out.topic=fruit-out\n"
"mp.messaging.outgoing.fruit-out.value.serializer=io.quarkus.kafka.client.serialization.ObjectMapperSerializer\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1627
msgid "Now, your Kafka messages will contain a Jackson serialized representation of your `Fruit` data object.  In this case, the `deserializer` configuration is not necessary as the xref:serialization-autodetection[Serializer/deserializer autodetection] is enabled by default."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1629
msgid "If you want to deserialize a list of fruits, you need to create a deserializer with a Jackson `TypeReference` denoted the generic collection used."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1637
#, no-wrap
msgid ""
"import java.util.List;\n"
"import com.fasterxml.jackson.core.type.TypeReference;\n"
"import io.quarkus.kafka.client.serialization.ObjectMapperDeserializer;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1643
#, no-wrap
msgid ""
"public class ListOfFruitDeserializer extends ObjectMapperDeserializer<List<Fruit>> {\n"
"    public ListOfFruitDeserializer() {\n"
"        super(new TypeReference<List<Fruit>>() {});\n"
"    }\n"
"}\n"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1646
#, no-wrap
msgid "Serializing via JSON-B"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1649
msgid "First, you need to include the `quarkus-jsonb` extension."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1657
#, no-wrap
msgid ""
"<dependency>\n"
"    <groupId>io.quarkus</groupId>\n"
"    <artifactId>quarkus-jsonb</artifactId>\n"
"</dependency>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1663
#, no-wrap
msgid "implementation(\"io.quarkus:quarkus-jsonb\")\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1667
msgid "There is an existing `JsonbSerializer` that can be used to serialize all data objects via JSON-B.  You may create an empty subclass if you want to use xref:serialization-autodetection[Serializer/deserializer autodetection]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1671
msgid "By default, the `JsonbSerializer` serializes null as the `\"null\"` String, this can be customized by setting the Kafka configuration property `json.serialize.null-as-null=true` which will serialize null as `null`.  This is handy when using a compacted topic, as `null` is used as a tombstone to know which messages delete during compaction phase."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1674
msgid "The corresponding deserializer class needs to be subclassed.  So, let's create a `FruitDeserializer` that extends the generic `JsonbDeserializer`."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1678
#, no-wrap
msgid "package com.acme.fruit.jsonb;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1680
#, no-wrap
msgid "import io.quarkus.kafka.client.serialization.JsonbDeserializer;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1686
#, no-wrap
msgid ""
"public class FruitDeserializer extends JsonbDeserializer<Fruit> {\n"
"    public FruitDeserializer() {\n"
"        super(Fruit.class);\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1689
msgid "Finally, configure your channels to use the JSON-B serializer and deserializer."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1696
#, no-wrap
msgid ""
"# Configure the Kafka source (we read from it)\n"
"mp.messaging.incoming.fruit-in.connector=smallrye-kafka\n"
"mp.messaging.incoming.fruit-in.topic=fruit-in\n"
"mp.messaging.incoming.fruit-in.value.deserializer=com.acme.fruit.jsonb.FruitDeserializer\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1701
#, no-wrap
msgid ""
"# Configure the Kafka sink (we write to it)\n"
"mp.messaging.outgoing.fruit-out.connector=smallrye-kafka\n"
"mp.messaging.outgoing.fruit-out.topic=fruit-out\n"
"mp.messaging.outgoing.fruit-out.value.serializer=io.quarkus.kafka.client.serialization.JsonbSerializer\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1704
msgid "Now, your Kafka messages will contain a JSON-B serialized representation of your `Fruit` data object."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1706
msgid "If you want to deserialize a list of fruits, you need to create a deserializer with a `Type` denoted the generic collection used."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1714
#, no-wrap
msgid ""
"package com.acme.fruit.jsonb;\n"
"import java.lang.reflect.Type;\n"
"import java.util.ArrayList;\n"
"import java.util.List;\n"
"import io.quarkus.kafka.client.serialization.JsonbDeserializer;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1720
#, no-wrap
msgid ""
"public class ListOfFruitDeserializer extends JsonbDeserializer<List<Fruit>> {\n"
"    public ListOfFruitDeserializer() {\n"
"        super(new ArrayList<MyEntity>() {}.getClass().getGenericSuperclass());\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1724
msgid "If you don't want to create a deserializer for each data object, you can use the generic `io.vertx.kafka.client.serialization.JsonObjectDeserializer` that will deserialize to a `io.vertx.core.json.JsonObject`. The corresponding serializer can also be used: `io.vertx.kafka.client.serialization.JsonObjectSerializer`."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1725
#, no-wrap
msgid "Avro Serialization"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1728
#: upstream/_versions/3.2/guides/kafka.adoc:1822
msgid "This is described in a dedicated guide: xref:kafka-schema-registry-avro.adoc[Using Apache Kafka with Schema Registry and Avro]."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1730
#, no-wrap
msgid "Serializer/deserializer autodetection"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1734
msgid "When using SmallRye Reactive Messaging with Kafka (`io.quarkus:quarkus-smallrye-reactive-messaging-kafka`), Quarkus can often automatically detect the correct serializer and deserializer class.  This autodetection is based on declarations of `@Incoming` and `@Outgoing` methods, as well as injected ``@Channel``s."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1736
msgid "For example, if you declare"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1743
#, no-wrap
msgid ""
"@Outgoing(\"generated-price\")\n"
"public Multi<Integer> generate() {\n"
"    ...\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1746
msgid "and your configuration indicates that the `generated-price` channel uses the `smallrye-kafka` connector, then Quarkus will automatically set the `value.serializer` to Kafka's built-in `IntegerSerializer`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1748
msgid "Similarly, if you declare"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1755
#, no-wrap
msgid ""
"@Incoming(\"my-kafka-records\")\n"
"public void consume(KafkaRecord<Long, byte[]> record) {\n"
"    ...\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1758
msgid "and your configuration indicates that the `my-kafka-records` channel uses the `smallrye-kafka` connector, then Quarkus will automatically set the `key.deserializer` to Kafka's built-in `LongDeserializer`, as well as the `value.deserializer` to `ByteArrayDeserializer`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1760
msgid "Finally, if you declare"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1766
#, no-wrap
msgid ""
"@Inject\n"
"@Channel(\"price-create\")\n"
"Emitter<Double> priceEmitter;\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1769
msgid "and your configuration indicates that the `price-create` channel uses the `smallrye-kafka` connector, then Quarkus will automatically set the `value.serializer` to Kafka's built-in `DoubleSerializer`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1771
msgid "The full set of types supported by the serializer/deserializer autodetection is:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1773
msgid "`short` and `java.lang.Short`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1774
msgid "`int` and `java.lang.Integer`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1775
msgid "`long` and `java.lang.Long`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1776
msgid "`float` and `java.lang.Float`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1777
msgid "`double` and `java.lang.Double`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1778
msgid "`byte[]`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1779
msgid "`java.lang.String`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1780
msgid "`java.util.UUID`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1781
msgid "`java.nio.ByteBuffer`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1782
msgid "`org.apache.kafka.common.utils.Bytes`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1783
msgid "`io.vertx.core.buffer.Buffer`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1784
msgid "`io.vertx.core.json.JsonObject`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1785
msgid "`io.vertx.core.json.JsonArray`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1786
msgid "classes for which a direct implementation of `org.apache.kafka.common.serialization.Serializer<T>` / `org.apache.kafka.common.serialization.Deserializer<T>` is present."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1787
msgid "the implementation needs to specify the type argument `T` as the (de-)serialized type."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1788
msgid "classes generated from Avro schemas, as well as Avro `GenericRecord`, if Confluent or Apicurio Registry _serde_ is present"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1789
msgid "in case multiple Avro serdes are present, serializer/deserializer must be configured manually for Avro-generated classes, because autodetection is impossible"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1790
msgid "see xref:kafka-schema-registry-avro.adoc[Using Apache Kafka with Schema Registry and Avro] for more information about using Confluent or Apicurio Registry libraries"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1791
msgid "classes for which a subclass of `ObjectMapperSerializer` / `ObjectMapperDeserializer` is present, as described in xref:jackson-serialization[Serializing via Jackson]"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1792
msgid "it is technically not needed to subclass `ObjectMapperSerializer`, but in such case, autodetection isn't possible"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1793
msgid "classes for which a subclass of `JsonbSerializer` / `JsonbDeserializer` is present, as described in xref:jsonb-serialization[Serializing via JSON-B]"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1794
msgid "it is technically not needed to subclass `JsonbSerializer`, but in such case, autodetection isn't possible"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1796
msgid "If a serializer/deserializer is set by configuration, it won't be replaced by the autodetection."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1799
msgid "In case you have any issues with serializer autodetection, you can switch it off completely by setting `quarkus.reactive-messaging.kafka.serializer-autodetection.enabled=false`.  If you find you need to do this, please file a bug in the link:https://github.com/quarkusio/quarkus/issues[Quarkus issue tracker] so we can fix whatever problem you have."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1801
#, no-wrap
msgid "JSON Serializer/deserializer generation"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1803
msgid "Quarkus automatically generates serializers and deserializers for channels where:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1805
msgid "the serializer/deserializer is not configured"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1806
msgid "the auto-detection did not find a matching serializer/deserializer"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1808
msgid "It uses Jackson underneath."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1810
msgid "This generation can be disabled using:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1814
#, no-wrap
msgid "quarkus.reactive-messaging.kafka.serializer-generation.enabled=false\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1818
msgid "Generation does not support collections such as `List<Fruit>`.  Refer to xref:jackson-serialization[Serializing via Jackson] to write your own serializer/deserializer for this case."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1819
#, no-wrap
msgid "Using Schema Registry"
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1824
#, no-wrap
msgid "Health Checks"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1828
msgid "Quarkus provides several health checks for Kafka.  These checks are used in combination with the `quarkus-smallrye-health` extension."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1829
#, no-wrap
msgid "Kafka Broker Readiness Check"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1834
msgid "When using the `quarkus-kafka-client` extension, you can enable _readiness_ health check by setting the `quarkus.kafka.health.enabled` property to `true` in your `application.properties`.  This check reports the status of the interaction with a _default_ Kafka broker (configured using `kafka.bootstrap.servers`).  It requires an _admin connection_ with the Kafka broker, and it is disabled by default.  If enabled, when you access the `/q/health/ready` endpoint of your application, you will have information about the connection validation status."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1835
#, no-wrap
msgid "Kafka Reactive Messaging Health Checks"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1837
msgid "When using Reactive Messaging and the Kafka connector, each configured channel (incoming or outgoing) provides _startup_, _liveness_ and _readiness_ checks."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1839
msgid "The _startup_ check verifies that the communication with Kafka cluster is established."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1840
msgid "The _liveness_ check captures any unrecoverable failure happening during the communication with Kafka."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1841
msgid "The _readiness_ check verifies that the Kafka connector is ready to consume/produce messages to the configured Kafka topics."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1843
msgid "For each channel, you can disable the checks using:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1847
#, no-wrap
msgid "# Disable both liveness and readiness checks with `health-enabled=false`:\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1852
#, no-wrap
msgid ""
"# Incoming channel (receiving records form Kafka)\n"
"mp.messaging.incoming.your-channel.health-enabled=false\n"
"# Outgoing channel (writing records to Kafka)\n"
"mp.messaging.outgoing.your-channel.health-enabled=false\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1854
#, no-wrap
msgid "# Disable only the readiness check with `health-readiness-enabled=false`:\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1857
#, no-wrap
msgid ""
"mp.messaging.incoming.your-channel.health-readiness-enabled=false\n"
"mp.messaging.outgoing.your-channel.health-readiness-enabled=false\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1861
msgid "You can configure the `bootstrap.servers` for each channel using `mp.messaging.incoming|outgoing.$channel.bootstrap.servers` property.  Default is `kafka.bootstrap.servers`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1865
msgid "Reactive Messaging _startup_ and _readiness_ checks offer two strategies.  The default strategy verifies that an active connection is established with the broker.  This approach is not intrusive as it's based on built-in Kafka client metrics."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1869
msgid "Using the `health-topic-verification-enabled=true` attribute, _startup_ probe uses an _admin client_ to check for the list of topics.  Whereas the _readiness_ probe for an incoming channel checks that at least one partition is assigned for consumption, and for an outgoing channel checks that the topic used by the producer exist in the broker."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1872
msgid "Note that to achieve this, an _admin connection_ is required.  You can adjust the timeout for topic verification calls to the broker using the `health-topic-verification-timeout` configuration."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1873
#, no-wrap
msgid "Kafka Streams"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1876
msgid "This is described in a dedicated guide: xref:kafka-streams.adoc[Using Apache Kafka Streams]."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1877
#, no-wrap
msgid "Using Snappy for message compression"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1880
msgid "On _outgoing_ channels, you can enable Snappy compression by setting the `compression.type` attribute to `snappy`:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1884
#, no-wrap
msgid "mp.messaging.outgoing.fruit-out.compression.type=snappy\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1888
msgid "In JVM mode, it will work out of the box.  However, to compile your application to a native executable, you need to:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1890
msgid "Uses GraalVM 21.+"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1891
msgid "Add `quarkus.kafka.snappy.enabled=true` to your `application.properties`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1893
msgid "In native mode, Snappy is disabled by default as the use of Snappy requires embedding a native library and unpacking it when the application starts."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1894
#, no-wrap
msgid "Authentication with OAuth"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1898
msgid "If your Kafka broker uses OAuth as authentication mechanism, you need to configure the Kafka consumer to enable this authentication process.  First, add the following dependency to your application:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1906
#, no-wrap
msgid ""
"<dependency>\n"
"    <groupId>io.strimzi</groupId>\n"
"    <artifactId>kafka-oauth-client</artifactId>\n"
"</dependency>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1912
#, no-wrap
msgid "implementation(\"io.strimzi:kafka-oauth-client\")\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1916
msgid "This dependency provides the callback handler required to handle the OAuth workflow.  Then, in the `application.properties`, add:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1926
#, no-wrap
msgid ""
"mp.messaging.connector.smallrye-kafka.security.protocol=SASL_PLAINTEXT\n"
"mp.messaging.connector.smallrye-kafka.sasl.mechanism=OAUTHBEARER\n"
"mp.messaging.connector.smallrye-kafka.sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \\\n"
"  oauth.client.id=\"team-a-client\" \\\n"
"  oauth.client.secret=\"team-a-client-secret\" \\\n"
"  oauth.token.endpoint.uri=\"http://keycloak:8080/auth/realms/kafka-authz/protocol/openid-connect/token\" ;\n"
"mp.messaging.connector.smallrye-kafka.sasl.login.callback.handler.class=io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1928
#, no-wrap
msgid "quarkus.ssl.native=true\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1931
msgid "Update the `oauth.client.id`, `oauth.client.secret` and `oauth.token.endpoint.uri` values."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1933
msgid "OAuth authentication works for both JVM and native modes. Since SSL in not enabled by default in native mode, `quarkus.ssl.native=true` must be added to support JaasClientOauthLoginCallbackHandler, which uses SSL. (See the xref:native-and-ssl.adoc[Using SSL with Native Executables] guide for more details.)"
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:1934
#, no-wrap
msgid "Testing a Kafka application"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:1936
#, no-wrap
msgid "Testing without a broker"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1940
msgid "It can be useful to test the application without having to start a Kafka broker.  To achieve this, you can _switch_ the channels managed by the Kafka connector to _in-memory_."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1942
msgid "This approach only works for JVM tests. It cannot be used for native tests (because they do not support injection)."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1944
msgid "Let's say we want to test the following processor application:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1949
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class BeverageProcessor {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1961
#, no-wrap
msgid ""
"    @Incoming(\"orders\")\n"
"    @Outgoing(\"beverages\")\n"
"    Beverage process(Order order) {\n"
"        System.out.println(\"Order received \" + order.getProduct());\n"
"        Beverage beverage = new Beverage();\n"
"        beverage.setBeverage(order.getProduct());\n"
"        beverage.setCustomer(order.getCustomer());\n"
"        beverage.setOrderId(order.getOrderId());\n"
"        beverage.setPreparationState(\"RECEIVED\");\n"
"        return beverage;\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1966
msgid "First, add the following test dependency to your application:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1975
#, no-wrap
msgid ""
"<dependency>\n"
"    <groupId>io.smallrye.reactive</groupId>\n"
"    <artifactId>smallrye-reactive-messaging-in-memory</artifactId>\n"
"    <scope>test</scope>\n"
"</dependency>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1981
#, no-wrap
msgid "testImplementation(\"io.smallrye.reactive:smallrye-reactive-messaging-in-memory\")\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:1984
msgid "Then, create a Quarkus Test Resource as follows:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1988
#, no-wrap
msgid "public class KafkaTestResourceLifecycleManager implements QuarkusTestResourceLifecycleManager {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:1998
#, no-wrap
msgid ""
"    @Override\n"
"    public Map<String, String> start() {\n"
"        Map<String, String> env = new HashMap<>();\n"
"        Map<String, String> props1 = InMemoryConnector.switchIncomingChannelsToInMemory(\"orders\");     // <1>\n"
"        Map<String, String> props2 = InMemoryConnector.switchOutgoingChannelsToInMemory(\"beverages\");  // <2>\n"
"        env.putAll(props1);\n"
"        env.putAll(props2);\n"
"        return env;  // <3>\n"
"    }\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2004
#, no-wrap
msgid ""
"    @Override\n"
"    public void stop() {\n"
"        InMemoryConnector.clear();  // <4>\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2006
msgid "Switch the incoming channel `orders` (expecting messages from Kafka) to in-memory."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2007
msgid "Switch the outgoing channel `beverages` (writing messages to Kafka) to in-memory."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2008
msgid "Builds and returns a `Map` containing all the properties required to configure the application to use in-memory channels."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2009
msgid "When the test stops, clear the `InMemoryConnector` (discard all the received and sent messages)"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2011
msgid "Create a Quarkus Test using the test resource created above:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2017
#, no-wrap
msgid ""
"@QuarkusTest\n"
"@QuarkusTestResource(KafkaTestResourceLifecycleManager.class)\n"
"class BaristaTest {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2020
#, no-wrap
msgid ""
"    @Inject\n"
"    InMemoryConnector connector; // <1>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2025
#, no-wrap
msgid ""
"    @Test\n"
"    void testProcessOrder() {\n"
"        InMemorySource<Order> ordersIn = connector.source(\"orders\");     // <2>\n"
"        InMemorySink<Beverage> beveragesOut = connector.sink(\"beverages\");  // <3>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2030
#, no-wrap
msgid ""
"        Order order = new Order();\n"
"        order.setProduct(\"coffee\");\n"
"        order.setName(\"Coffee lover\");\n"
"        order.setOrderId(\"1234\");\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2032
#, no-wrap
msgid "        ordersIn.send(order);  // <4>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2034
#, no-wrap
msgid "        await().<List<? extends Message<Beverage>>>until(beveragesOut::received, t -> t.size() == 1); // <5>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2041
#, no-wrap
msgid ""
"        Beverage queuedBeverage = beveragesOut.received().get(0).getPayload();\n"
"        Assertions.assertEquals(Beverage.State.READY, queuedBeverage.getPreparationState());\n"
"        Assertions.assertEquals(\"coffee\", queuedBeverage.getBeverage());\n"
"        Assertions.assertEquals(\"Coffee lover\", queuedBeverage.getCustomer());\n"
"        Assertions.assertEquals(\"1234\", queuedBeverage.getOrderId());\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2045
msgid "Inject the in-memory connector in your test class."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2046
msgid "Retrieve the incoming channel (`orders`) - the channel must have been switched to in-memory in the test resource."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2047
msgid "Retrieve the outgoing channel (`beverages`) - the channel must have been switched to in-memory in the test resource."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2049
msgid "Use the `send` method to send a message to the `orders` channel.  The application will process this message and send a message to `beverages` channel."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2050
msgid "Use the `received` method on `beverages` channel to check the messages produced by the application."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:2055
msgid "With in-memory channels we were able to test application code processing messages without starting a Kafka broker.  Note that different in-memory channels are independent, and switching channel connector to in-memory does not simulate message delivery between channels configured to the same Kafka topic."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2057
#, no-wrap
msgid "Testing using a Kafka broker"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2062
msgid "If you are using xref:kafka-dev-services[Dev Services for Kafka], a Kafka broker will be started and available throughout the tests, unless it is disabled in `%test` profile.  While it is possible to connect to this broker using Kafka Clients API, https://smallrye.io/smallrye-reactive-messaging/latest/kafka/test-companion/[Kafka Companion Library] proposes an easier way of interacting with a Kafka broker and, creating consumer, producer and admin actions inside tests."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2064
msgid "For using `KafkaCompanion` API in tests, start by adding the following dependency:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2072
#, no-wrap
msgid ""
"<dependency>\n"
"    <groupId>io.quarkus</groupId>\n"
"    <artifactId>quarkus-test-kafka-companion</artifactId>\n"
"    <scope>test</scope>\n"
"</dependency>\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2075
msgid "which provides `io.quarkus.test.kafka.KafkaCompanionResource` - an implementation of `io.quarkus.test.common.QuarkusTestResourceLifecycleManager`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2077
msgid "Then use `@QuarkusTestResource` to configure the Kafka Companion in tests, for example:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2081
#, no-wrap
msgid "import static org.junit.jupiter.api.Assertions.assertEquals;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2083
#, no-wrap
msgid "import java.util.UUID;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2086
#, no-wrap
msgid ""
"import org.apache.kafka.clients.producer.ProducerRecord;\n"
"import org.junit.jupiter.api.Test;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2093
#, no-wrap
msgid ""
"import io.quarkus.test.common.QuarkusTestResource;\n"
"import io.quarkus.test.junit.QuarkusTest;\n"
"import io.quarkus.test.kafka.InjectKafkaCompanion;\n"
"import io.quarkus.test.kafka.KafkaCompanionResource;\n"
"import io.smallrye.reactive.messaging.kafka.companion.ConsumerTask;\n"
"import io.smallrye.reactive.messaging.kafka.companion.KafkaCompanion;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2097
#, no-wrap
msgid ""
"@QuarkusTest\n"
"@QuarkusTestResource(KafkaCompanionResource.class)\n"
"public class OrderProcessorTest {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2100
#, no-wrap
msgid ""
"    @InjectKafkaCompanion // <1>\n"
"    KafkaCompanion companion;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2104
#, no-wrap
msgid ""
"    @Test\n"
"    void testProcessor() {\n"
"        companion.produceStrings().usingGenerator(i -> new ProducerRecord<>(\"orders\", UUID.randomUUID().toString())); // <2>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2106
#, no-wrap
msgid "        // Expect that the tested application processes orders from 'orders' topic and write to 'orders-processed' topic\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2112
#, no-wrap
msgid ""
"        ConsumerTask<String, String> orders = companion.consumeStrings().fromTopics(\"orders-processed\", 10); // <3>\n"
"        orders.awaitCompletion(); // <4>\n"
"        assertEquals(10, orders.count());\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2115
msgid "`@InjectKafkaCompanion` injects the `KafkaCompanion` instance, configured to access the Kafka broker created for tests."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2116
msgid "Use `KafkaCompanion` to create producer task which writes 10 records to 'orders' topic."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2117
msgid "Create consumer task which subscribes to 'orders-processed' topic and consumes 10 records."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2118
msgid "Await completion of the consumer task."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:2122
msgid "If the Kafka Dev Service is available during tests, `KafkaCompanionResource` uses the created Kafka broker, otherwise it creates a Kafka broker using https://github.com/strimzi/test-container[Strimzi Test Container]."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:2124
msgid "The configuration of the created Kafka broker can be customized using `@ResourceArg`, for example:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2135
#, no-wrap
msgid ""
"@QuarkusTestResource(value = KafkaCompanionResource.class, initArgs = {\n"
"        @ResourceArg(name = \"strimzi.kafka.image\", value = \"quay.io/strimzi/kafka:0.28.0-kafka-3.0.0\"), // Image name\n"
"        @ResourceArg(name = \"kafka.port\", value = \"9092\"), // Fixed port for kafka, by default it will be exposed on a random port\n"
"        @ResourceArg(name = \"kraft\", value = \"true\"), // Enable Kraft mode\n"
"        @ResourceArg(name = \"num.partitions\", value = \"3\"), // Other custom broker configurations\n"
"})\n"
"public class OrderProcessorTest {\n"
"    // ...\n"
"}\n"
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:2138
#, no-wrap
msgid "Custom test resource"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:2142
msgid "Alternatively, you can start a Kafka broker in a test resource.  The following snippet shows a test resource starting a Kafka broker using https://www.testcontainers.org/modules/kafka/[Testcontainers]:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2146
#, no-wrap
msgid "public class KafkaResource implements QuarkusTestResourceLifecycleManager {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2148
#, no-wrap
msgid "    private final KafkaContainer kafka = new KafkaContainer();\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2154
#, no-wrap
msgid ""
"    @Override\n"
"    public Map<String, String> start() {\n"
"        kafka.start();\n"
"        return Collections.singletonMap(\"kafka.bootstrap.servers\", kafka.getBootstrapServers());  // <1>\n"
"    }\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2160
#, no-wrap
msgid ""
"    @Override\n"
"    public void stop() {\n"
"        kafka.close();\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2162
msgid "Configure the Kafka bootstrap location, so the application connects to this broker."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:2169
#, no-wrap
msgid "Kubernetes Service Bindings"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2174
msgid "Quarkus Kafka extension supports xref:deploying-to-kubernetes.adoc[Service Binding Specification for Kubernetes].  You can enable this by adding the `quarkus-kubernetes-service-binding` extension to your application."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2176
msgid "When running in appropriately configured Kubernetes clusters, Kafka extension will pull its Kafka broker connection configuration from the service binding available inside the cluster, without the need for user configuration."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:2177
#, no-wrap
msgid "Execution model"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2182
msgid "Reactive Messaging invokes user's methods on an I/O thread.  Thus, by default, the methods must not block.  As described in xref:blocking-processing[Blocking processing], you need to add the `@Blocking` annotation on the method if this method will block the caller thread."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2184
msgid "See the xref:quarkus-reactive-architecture.adoc[Quarkus Reactive Architecture documentation] for further details on this topic."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:2185
#, no-wrap
msgid "Channel Decorators"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2188
msgid "SmallRye Reactive Messaging supports decorating incoming and outgoing channels for implementing cross-cutting concerns such as monitoring, tracing or message interception. For more information on implementing decorators and message interceptors see the http://smallrye.io/smallrye-reactive-messaging/3.19.1/concepts/decorators/[SmallRye Reactive Messaging documentation]."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:2190
#, no-wrap
msgid "Configuration Reference"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2193
msgid "More details about the SmallRye Reactive Messaging configuration can be found in the https://smallrye.io/smallrye-reactive-messaging/latest/kafka/kafka/#using-the-kafka-connector[SmallRye Reactive Messaging - Kafka Connector Documentation]."
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:2197
msgid "Each channel can be disabled via configuration using:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2201
#, no-wrap
msgid "mp.messaging.[incoming|outgoing].[channel].enabled=false\n"
msgstr ""

#. type: delimited block =
#: upstream/_versions/3.2/guides/kafka.adoc:2205
msgid "The most important attributes are listed in the tables below:"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2206
#, no-wrap
msgid "Incoming channel configuration (polling from Kafka)"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2209
#: upstream/_versions/3.2/guides/kafka.adoc:2248
msgid "The following attributes are configured using:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2213
#, no-wrap
msgid "mp.messaging.incoming.your-channel-name.attribute=value\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2216
#: upstream/_versions/3.2/guides/kafka.adoc:2255
msgid "Some properties have aliases which can be configured globally:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2220
#: upstream/_versions/3.2/guides/kafka.adoc:2259
#, no-wrap
msgid "kafka.bootstrap.servers=...\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2223
msgid "You can also pass any property supported by the underlying https://kafka.apache.org/documentation/#consumerconfigs[Kafka consumer]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2225
msgid "For example, to configure the `max.poll.records` property, use:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2229
#, no-wrap
msgid "mp.messaging.incoming.[channel].max.poll.records=1000\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2232
msgid "Some consumer client properties are configured to sensible default values:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2234
#: upstream/_versions/3.2/guides/kafka.adoc:2273
msgid "If not set, `reconnect.backoff.max.ms` is set to `10000` to avoid high load on disconnection."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2236
msgid "If not set, `key.deserializer` is set to `org.apache.kafka.common.serialization.StringDeserializer`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2238
msgid "The consumer `client.id` is configured according to the number of clients to create using `mp.messaging.incoming.[channel].partitions` property."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2240
msgid "If a `client.id` is provided, it is used as-is or suffixed with client index if `partitions` property is set."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2241
msgid "If a `client.id` is not provided, it is generated as `[client-id-prefix][channel-name][-index]`."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2245
#, no-wrap
msgid "Outgoing channel configuration (writing to Kafka)"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2252
#, no-wrap
msgid "mp.messaging.outgoing.your-channel-name.attribute=value\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2262
msgid "You can also pass any property supported by the underlying https://kafka.apache.org/documentation/#producerconfigs[Kafka producer]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2264
msgid "For example, to configure the `max.block.ms` property, use:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2268
#, no-wrap
msgid "mp.messaging.incoming.[channel].max.block.ms=10000\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2271
msgid "Some producer client properties are configured to sensible default values:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2275
msgid "If not set, `key.serializer` is set to `org.apache.kafka.common.serialization.StringSerializer`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2277
msgid "If not set, producer `client.id` is generated as `[client-id-prefix][channel-name]`."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2281
#, no-wrap
msgid "Kafka Configuration Resolution"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2285
msgid "Quarkus exposes all Kafka related application properties, prefixed with `kafka.` or `KAFKA_` inside a configuration map with `default-kafka-broker` name.  This configuration is used to establish the connection with the Kafka broker."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2287
msgid "In addition to this default configuration, you can configure the name of the `Map` producer using the `kafka-configuration` attribute:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2292
#, no-wrap
msgid ""
"mp.messaging.incoming.my-channel.connector=smallrye-kafka\n"
"mp.messaging.incoming.my-channel.kafka-configuration=my-configuration\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2296
msgid "In this case, the connector looks for the `Map` associated with the `my-configuration` name.  If `kafka-configuration` is not set, an optional lookup for a `Map` exposed with the channel name (`my-channel` in the previous example) is done."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2307
#, no-wrap
msgid ""
"@Produces\n"
"@ApplicationScoped\n"
"@Identifier(\"my-configuration\")\n"
"Map<String, Object> outgoing() {\n"
"    return Map.ofEntries(\n"
"            Map.entry(\"value.serializer\", ObjectMapperSerializer.class.getName())\n"
"    );\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2310
msgid "If `kafka-configuration` is set and no `Map` can be found, the deployment fails."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2312
msgid "Attribute values are resolved as follows:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2314
msgid "the attribute is set directly on the channel configuration (`mp.messaging.incoming.my-channel.attribute=value`),"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2315
msgid "if not set, the connector looks for a `Map` with the channel name or the configured `kafka-configuration` (if set) and the value is retrieved from that `Map`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2316
msgid "If the resolved `Map` does not contain the value the default `Map` is used (exposed with the `default-kafka-broker` name)"
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:2317
#, no-wrap
msgid "Integrating with Kafka - Common patterns"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2319
#, no-wrap
msgid "Writing to Kafka from an HTTP endpoint"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2322
msgid "To send messages to Kafka from an HTTP endpoint, inject an `Emitter` (or a `MutinyEmitter`) in your endpoint:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2333
#: upstream/_versions/3.2/guides/kafka.adoc:2381
#, no-wrap
msgid ""
"import jakarta.ws.rs.POST;\n"
"import jakarta.ws.rs.Path;\n"
"import jakarta.ws.rs.Produces;\n"
"import jakarta.ws.rs.core.MediaType;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2339
#: upstream/_versions/3.2/guides/kafka.adoc:2389
#: upstream/_versions/3.2/guides/kafka.adoc:2614
#, no-wrap
msgid ""
"@Path(\"/\")\n"
"public class ResourceSendingToKafka {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2341
#, no-wrap
msgid "    @Channel(\"kafka\") Emitter<String> emitter;          // <1>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2348
#, no-wrap
msgid ""
"    @POST\n"
"    @Produces(MediaType.TEXT_PLAIN)\n"
"    public CompletionStage<Void> send(String payload) { // <2>\n"
"        return emitter.send(payload);                   // <3>\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2350
msgid "Inject an `Emitter<String>`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2351
msgid "The HTTP method receives the payload and returns a `CompletionStage` completed when the message is written to Kafka"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2352
msgid "Send the message to Kafka, the `send` method returns a `CompletionStage`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2355
msgid "The endpoint sends the passed payload (from a `POST` HTTP request) to the emitter.  The emitter's channel is mapped to a Kafka topic in the `application.properties` file:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2360
#, no-wrap
msgid ""
"mp.messaging.outgoing.kafka.connector=smallrye-kafka\n"
"mp.messaging.outgoing.kafka.topic=my-topic\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2366
msgid "The endpoint returns a `CompletionStage` indicating the asynchronous nature of the method.  The `emitter.send` method returns a `CompletionStage<Void>` .  The returned future is completed when the message has been written to Kafka.  If the writing fails, the returned `CompletionStage` is completed exceptionally."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2368
msgid "If the endpoint does not return a `CompletionStage`, the HTTP response may be written before the message is sent to Kafka, and so failures won't be reported to the user."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2370
msgid "If you need to send a Kafka record, use:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2386
#, no-wrap
msgid "import io.smallrye.reactive.messaging.kafka.Record;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2391
#, no-wrap
msgid "    @Channel(\"kafka\") Emitter<Record<String,String>> emitter;  // <1>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2399
#, no-wrap
msgid ""
"    @POST\n"
"    @Produces(MediaType.TEXT_PLAIN)\n"
"    public CompletionStage<Void> send(String payload) {\n"
"        return emitter.send(Record.of(\"my-key\", payload));    // <2>\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2401
msgid "Note the usage of an `Emitter<Record<K, V>>`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2402
msgid "Create the record using `Record.of(k, v)`"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2403
#, no-wrap
msgid "Persisting Kafka messages with Hibernate with Panache"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2406
msgid "To persist objects received from Kafka into a database, you can use Hibernate with Panache."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2408
msgid "If you use Hibernate Reactive, look at xref:persisting-kafka-messages-with-hibernate-reactive[Persisting Kafka messages with Hibernate Reactive]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2411
#: upstream/_versions/3.2/guides/kafka.adoc:2491
msgid "Let's imagine you receive `Fruit` objects.  For simplicity purposes, our `Fruit` class is pretty simple:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2419
#, no-wrap
msgid "import io.quarkus.hibernate.orm.panache.PanacheEntity;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2422
#: upstream/_versions/3.2/guides/kafka.adoc:2502
#, no-wrap
msgid ""
"@Entity\n"
"public class Fruit extends PanacheEntity {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2424
#: upstream/_versions/3.2/guides/kafka.adoc:2504
#, no-wrap
msgid "    public String name;\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2429
#: upstream/_versions/3.2/guides/kafka.adoc:2510
msgid "To consume `Fruit` instances stored on a Kafka topic, and persist them into a database, you can use the following approach:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2440
#, no-wrap
msgid "import io.smallrye.common.annotation.Blocking;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2443
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class FruitConsumer {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2450
#, no-wrap
msgid ""
"    @Incoming(\"fruits\")                                     // <1>\n"
"    @Transactional                                          // <2>\n"
"    public void persistFruits(Fruit fruit) {                // <3>\n"
"        fruit.persist();                                    // <4>\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2452
msgid "Configuring the incoming channel. This channel reads from Kafka."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2454
msgid "As we are writing in a database, we must be in a transaction. This annotation starts a new transaction and commits it when the method returns.  Quarkus automatically considers the method as _blocking_. Indeed, writing to a database using classic Hibernate is blocking. So, Quarkus calls the method on a worker thread you can block (and not an I/O thread)."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2455
msgid "The method receives each Fruit. Note that you would need a deserializer to reconstruct the Fruit instances from the Kafka records."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2456
msgid "Persist the received `fruit` object."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2459
msgid "As mentioned in <4>, you need a deserializer that can create a `Fruit` from the record.  This can be done using a Jackson deserializer:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2474
#: upstream/_versions/3.2/guides/kafka.adoc:2570
msgid "The associated configuration would be:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2479
#: upstream/_versions/3.2/guides/kafka.adoc:2575
#, no-wrap
msgid ""
"mp.messaging.incoming.fruits.connector=smallrye-kafka\n"
"mp.messaging.incoming.fruits.value.deserializer=org.acme.FruitDeserializer\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2483
#: upstream/_versions/3.2/guides/kafka.adoc:2579
msgid "Check xref:jackson-serialization[Serializing via Jackson] for more detail about the usage of Jackson with Kafka.  You can also use Avro."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2485
#, no-wrap
msgid "Persisting Kafka messages with Hibernate Reactive"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2488
msgid "To persist objects received from Kafka into a database, you can use Hibernate Reactive with Panache."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2499
#, no-wrap
msgid "import io.quarkus.hibernate.reactive.panache.PanacheEntity;  // <1>\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2508
msgid "Make sure to use the reactive variant"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2517
#, no-wrap
msgid ""
"import jakarta.enterprise.context.ApplicationScoped;\n"
"import jakarta.enterprise.context.control.ActivateRequestContext;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2522
#, no-wrap
msgid ""
"import io.quarkus.hibernate.reactive.panache.Panache;\n"
"import io.smallrye.mutiny.Uni;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2525
#, no-wrap
msgid ""
"@ApplicationScoped\n"
"public class FruitStore {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2528
#, no-wrap
msgid ""
"    @Inject\n"
"    Mutiny.Session session;                    // <1>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2537
#, no-wrap
msgid ""
"    @Incoming(\"in\")\n"
"    @ActivateRequestContext // <2>\n"
"    public Uni<Void> consume(Fruit entity) {\n"
"        return session.withTransaction(t -> {  // <3>\n"
"            return entity.persistAndFlush()    // <4>\n"
"                    .replaceWithVoid();        // <5>\n"
"        }).onTermination().call(() -> session.close()); // <6>\n"
"    }\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2541
msgid "Inject the Hibernate Reactive `Session`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2544
msgid "Hibernate Reactive `Session` and `Panache` APIs require an active CDI Request context.  `@ActivateRequestContext` annotation creates a new request context and destroys it when the `Uni` returned from the method completes.  If `Panache` is not used, `Mutiny.SessionFactory` can be injected and used similarly without the need of activating the request context or closing the session manually."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2545
msgid "Requests a new transaction. The transaction completes when the passed action completes."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2546
msgid "Persist the entity. It returns a `Uni<Fruit>`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2547
msgid "Switch back to a `Uni<Void>`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2548
msgid "Close the session - this is close the connection with the database. The connection can then be recycled."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2552
msgid "Unlike with _classic_ Hibernate, you can't use `@Transactional`.  Instead, we use `session.withTransaction` and persist our entity.  The `map` is used to return a `Uni<Void>` and not a `Uni<Fruit>`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2555
msgid "You need a deserializer that can create a `Fruit` from the record.  This can be done using a Jackson deserializer:"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2580
#, no-wrap
msgid "Writing entities managed by Hibernate to Kafka"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2583
msgid "Let's imagine the following process:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2585
msgid "You receive an HTTP request with a payload,"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2586
msgid "You create an Hibernate entity instance from this payload,"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2587
msgid "You persist that entity into a database,"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2588
msgid "You send the entity to a Kafka topic"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2590
msgid "If you use Hibernate Reactive, look at xref:writing-entities-managed-by-hibernate-reactive-to-kafka[Writing entities managed by Hibernate Reactive to Kafka]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2596
msgid "Because we write to a database, we must run this method in a transaction.  Yet, sending the entity to Kafka happens asynchronously.  The operation returns a `CompletionStage` (or a `Uni` if you use a `MutinyEmitter`) reporting when the operation completes.  We must be sure that the transaction is still running until the object is written.  Otherwise, you may access the object outside the transaction, which is not allowed."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2598
msgid "To implement this process, you need the following approach:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2608
#, no-wrap
msgid ""
"import jakarta.transaction.Transactional;\n"
"import jakarta.ws.rs.POST;\n"
"import jakarta.ws.rs.Path;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2616
#, no-wrap
msgid "    @Channel(\"kafka\") Emitter<Fruit> emitter;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2625
#, no-wrap
msgid ""
"    @POST\n"
"    @Path(\"/fruits\")\n"
"    @Transactional                                                      // <1>\n"
"    public CompletionStage<Void> storeAndSendToKafka(Fruit fruit) {     // <2>\n"
"        fruit.persist();\n"
"        return emitter.send(new FruitDto(fruit));                       // <3>\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2627
msgid "As we are writing to the database, make sure we run inside a transaction"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2628
msgid "The method receives the fruit instance to persist. It returns a `CompletionStage` which is used for the transaction demarcation. The transaction is committed when the return `CompletionStage` completes. In our case, it's when the message is written to Kafka."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2630
msgid "Wrap the managed entity inside a Data transfer object and send it to Kafka.  This makes sure that managed entity is not impacted by the Kafka serialization."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2632
#, no-wrap
msgid "Writing entities managed by Hibernate Reactive to Kafka"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2635
msgid "To send to Kafka entities managed by Hibernate Reactive, we recommend using:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2637
msgid "RESTEasy Reactive to serve HTTP requests"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2638
msgid "A `MutinyEmitter` to send message to a channel, so it can be easily integrated with the Mutiny API exposed by Hibernate Reactive or Hibernate Reactive with Panache."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2640
msgid "The following example demonstrates how to receive a payload, store it in the database using Hibernate Reactive with Panache, and send the persisted entity to Kafka:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2647
#, no-wrap
msgid ""
"import jakarta.ws.rs.POST;\n"
"import jakarta.ws.rs.Path;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2653
#, no-wrap
msgid ""
"import io.quarkus.hibernate.reactive.panache.Panache;\n"
"import io.smallrye.mutiny.Uni;\n"
"import io.smallrye.reactive.messaging.MutinyEmitter;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2656
#, no-wrap
msgid ""
"@Path(\"/\")\n"
"public class ReactiveGreetingResource {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2658
#, no-wrap
msgid "    @Channel(\"kafka\") MutinyEmitter<Fruit> emitter;     // <1>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2668
#, no-wrap
msgid ""
"    @POST\n"
"    @Path(\"/fruits\")\n"
"    public Uni<Void> sendToKafka(Fruit fruit) {         // <2>\n"
"        return Panache.withTransaction(() ->            // <3>\n"
"            fruit.<Fruit>persist()\n"
"        )\n"
"            .chain(f -> emitter.send(f));               // <4>\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2670
msgid "Inject a `MutinyEmitter` which exposes a Mutiny API. It simplifies the integration with the Mutiny API exposed by Hibernate Reactive with Panache."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2671
msgid "The HTTP method receiving the payload returns a `Uni<Void>`. The HTTP response is written when the operation completes (the entity is persisted and written to Kafka)."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2672
msgid "We need to write the entity into the database in a transaction."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2673
msgid "Once the persist operation completes, we send the entity to Kafka. The `send` method returns a `Uni<Void>`."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2675
#, no-wrap
msgid "Streaming Kafka topics as server-sent events"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2678
msgid "Streaming a Kafka topic as server-sent events (SSE) is straightforward:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2680
msgid "You inject the channel representing the Kafka topic in your HTTP endpoint"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2681
msgid "You return that channel as a `Publisher` or a `Multi` from the HTTP method"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2683
msgid "The following code provides an example:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2688
#: upstream/_versions/3.2/guides/kafka.adoc:2703
#, no-wrap
msgid ""
"@Channel(\"fruits\")\n"
"Multi<Fruit> fruits;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2694
#, no-wrap
msgid ""
"@GET\n"
"@Produces(MediaType.SERVER_SENT_EVENTS)\n"
"public Multi<Fruit> stream() {\n"
"    return fruits;\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2698
msgid "Some environment cuts the SSE connection when there is not enough activity.  The workaround consists of sending _ping_ messages (or empty objects) periodically."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2706
#, no-wrap
msgid ""
"@Inject\n"
"ObjectMapper mapper;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2716
#, no-wrap
msgid ""
"@GET\n"
"@Produces(MediaType.SERVER_SENT_EVENTS)\n"
"public Multi<String> stream() {\n"
"    return Multi.createBy().merging()\n"
"            .streams(\n"
"                    fruits.map(this::toJson),\n"
"                    emitAPeriodicPing()\n"
"            );\n"
"}\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2721
#, no-wrap
msgid ""
"Multi<String> emitAPeriodicPing() {\n"
"    return Multi.createFrom().ticks().every(Duration.ofSeconds(10))\n"
"            .onItem().transform(x -> \"{}\");\n"
"}\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2729
#, no-wrap
msgid ""
"private String toJson(Fruit f) {\n"
"    try {\n"
"        return mapper.writeValueAsString(f);\n"
"    } catch (JsonProcessingException e) {\n"
"        throw new RuntimeException(e);\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2733
msgid "The workaround is a bit more complex as besides sending the fruits coming from Kafka, we need to send pings periodically.  To achieve this we merge the stream coming from Kafka and a periodic stream emitting `{}` every 10 seconds."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2735
#, no-wrap
msgid "Chaining Kafka Transactions with Hibernate Reactive transactions"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2739
msgid "By chaining a Kafka transaction with a Hibernate Reactive transaction you can send records to a Kafka transaction, perform database updates and commit the Kafka transaction only if the database transaction is successful."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2741
msgid "The following example demonstrates:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2743
msgid "Receive a payload by serving HTTP requests using RESTEasy Reactive,"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2744
msgid "Limit concurrency of that HTTP endpoint using Smallrye Fault Tolerance,"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2745
msgid "Start a Kafka transaction and send the payload to Kafka record,"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2746
msgid "Store the payload in the database using Hibernate Reactive with Panache,"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2747
msgid "Commit the Kafka transaction only if the entity is persisted successfully."
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2756
#, no-wrap
msgid ""
"import jakarta.ws.rs.Consumes;\n"
"import jakarta.ws.rs.POST;\n"
"import jakarta.ws.rs.Path;\n"
"import jakarta.ws.rs.core.MediaType;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2760
#: upstream/_versions/3.2/guides/kafka.adoc:2810
#, no-wrap
msgid ""
"import org.eclipse.microprofile.faulttolerance.Bulkhead;\n"
"import org.eclipse.microprofile.reactive.messaging.Channel;\n"
"import org.hibernate.reactive.mutiny.Mutiny;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2764
#, no-wrap
msgid ""
"import io.quarkus.hibernate.reactive.panache.Panache;\n"
"import io.smallrye.mutiny.Uni;\n"
"import io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2767
#: upstream/_versions/3.2/guides/kafka.adoc:2818
#, no-wrap
msgid ""
"@Path(\"/\")\n"
"public class FruitProducer {\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2769
#, no-wrap
msgid "    @Channel(\"kafka\") KafkaTransactions<Fruit> kafkaTx; // <1>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2783
#, no-wrap
msgid ""
"    @POST\n"
"    @Path(\"/fruits\")\n"
"    @Consumes(MediaType.APPLICATION_JSON)\n"
"    @Bulkhead(1) // <2>\n"
"    public Uni<Void> post(Fruit fruit) { // <3>\n"
"        return kafkaTx.withTransaction(emitter -> { // <4>\n"
"            emitter.send(fruit); // <5>\n"
"            return Panache.withTransaction(() -> { // <6>\n"
"                return fruit.<Fruit>persist(); // <7>\n"
"            });\n"
"        }).replaceWithVoid();\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2786
msgid "Inject a `KafkaTransactions` which exposes a Mutiny API. It allows the integration with the Mutiny API exposed by Hibernate Reactive with Panache."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2787
msgid "Limit the concurrency of the HTTP endpoint to \"1\", preventing starting multiple transactions at a given time."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2788
msgid "The HTTP method receiving the payload returns a `Uni<Void>`. The HTTP response is written when the operation completes (the entity is persisted and Kafka transaction is committed)."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2789
#: upstream/_versions/3.2/guides/kafka.adoc:2842
msgid "Begin a Kafka transaction."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2790
msgid "Send the payload to Kafka inside the Kafka transaction."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2791
msgid "Persist the entity into the database in a Hibernate Reactive transaction."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2793
msgid "Once the persist operation completes, and there is no errors, the Kafka transaction is committed.  The result is omitted and returned as the HTTP response."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2796
msgid "In the previous example the database transaction (inner) will commit followed by the Kafka transaction (outer).  If you wish to commit the Kafka transaction first and the database transaction second, you need to nest them in the reverse order."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2798
msgid "The next example demonstrates that using the Hibernate Reactive API (without Panache):"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2806
#, no-wrap
msgid ""
"import jakarta.inject.Inject;\n"
"import jakarta.ws.rs.Consumes;\n"
"import jakarta.ws.rs.POST;\n"
"import jakarta.ws.rs.Path;\n"
"import jakarta.ws.rs.core.MediaType;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2815
#, no-wrap
msgid ""
"import io.smallrye.mutiny.Uni;\n"
"import io.smallrye.reactive.messaging.kafka.transactions.KafkaTransactions;\n"
"import io.vertx.mutiny.core.Context;\n"
"import io.vertx.mutiny.core.Vertx;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2820
#, no-wrap
msgid "    @Channel(\"kafka\") KafkaTransactions<Fruit> kafkaTx;\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2822
#, no-wrap
msgid "    @Inject Mutiny.SessionFactory sf; // <1>\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2836
#, no-wrap
msgid ""
"    @POST\n"
"    @Path(\"/fruits\")\n"
"    @Consumes(MediaType.APPLICATION_JSON)\n"
"    @Bulkhead(1)\n"
"    public Uni<Void> post(Fruit fruit) {\n"
"        Context context = Vertx.currentContext(); // <2>\n"
"        return sf.withTransaction(session -> // <3>\n"
"                kafkaTx.withTransaction(emitter -> // <4>\n"
"                        session.persist(fruit).invoke(() -> emitter.send(fruit)) // <5>\n"
"                ).emitOn(context::runOnContext) // <6>\n"
"        );\n"
"    }\n"
"}\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2839
msgid "Inject the Hibernate Reactive `SessionFactory`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2840
msgid "Capture the caller Vert.x context."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2841
msgid "Begin a Hibernate Reactive transaction."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2843
msgid "Persist the payload and send the entity to Kafka."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2845
msgid "The Kafka transaction terminates on the Kafka producer sender thread.  We need to switch to the Vert.x context previously captured in order to terminate the Hibernate Reactive transaction on the same context we started it."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:2846
#, no-wrap
msgid "Logging"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2849
msgid "To reduce the amount of log written by the Kafka client, Quarkus sets the level of the following log categories to `WARNING`:"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2851
msgid "`org.apache.kafka.clients`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2852
msgid "`org.apache.kafka.common.utils`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2853
msgid "`org.apache.kafka.common.metrics`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2855
msgid "You can override the configuration by adding the following lines to the `application.properties`:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2861
#, no-wrap
msgid ""
"quarkus.log.category.\"org.apache.kafka.clients\".level=INFO\n"
"quarkus.log.category.\"org.apache.kafka.common.utils\".level=INFO\n"
"quarkus.log.category.\"org.apache.kafka.common.metrics\".level=INFO\n"
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:2863
#, no-wrap
msgid "Connecting to Managed Kafka clusters"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2866
msgid "This section explains how to connect to notorious Kafka Cloud Services."
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2867
#, no-wrap
msgid "Azure Event Hub"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2870
msgid "https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview[Azure Event Hub] provides an endpoint compatible with Apache Kafka."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2874
msgid "Azure Event Hubs for Kafka is not available in the _basic_ tier.  You need at least the _standard_ tier to use Kafka.  See https://azure.microsoft.com/en-us/pricing/details/event-hubs/[Azure Event Hubs Pricing] to see the other options."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2876
msgid "To connect to Azure Event Hub, using the Kafka protocol with TLS, you need the following configuration:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2885
#, no-wrap
msgid ""
"kafka.bootstrap.servers=my-event-hub.servicebus.windows.net:9093 # <1>\n"
"kafka.security.protocol=SASL_SSL\n"
"kafka.sasl.mechanism=PLAIN\n"
"kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\ # <2>\n"
"    username=\"$ConnectionString\" \\ # <3>\n"
"    password=\"<YOUR.EVENTHUBS.CONNECTION.STRING>\"; # <4>\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2887
msgid "The port is `9093`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2888
msgid "You need to use the JAAS `PlainLoginModule`."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2889
msgid "The username is the `$ConnectionString` string."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2890
msgid "The Event Hub connection string given by Azure."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2894
msgid "Replace `<YOUR.EVENTHUBS.CONNECTION.STRING>` with the connection string for your Event Hubs namespace.  For instructions on getting the connection string, see https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-get-connection-string[Get an Event Hubs connection string].  The result would be something like:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2900
#, no-wrap
msgid ""
"kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\\n"
"    username=\"$ConnectionString\" \\\n"
"    password=\"Endpoint=sb://my-event-hub.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=XXXXXXXXXXXXXXXX\";\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2903
msgid "This configuration can be global (as above), or set in the channel configuration:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2912
#, no-wrap
msgid ""
"mp.messaging.incoming.$channel.bootstrap.servers=my-event-hub.servicebus.windows.net:9093\n"
"mp.messaging.incoming.$channel.security.protocol=SASL_SSL\n"
"mp.messaging.incoming.$channel.sasl.mechanism=PLAIN\n"
"mp.messaging.incoming.$channel.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\\n"
"    username=\"$ConnectionString\" \\\n"
"    password=\"Endpoint=sb://my-event-hub.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=...\";\n"
msgstr ""

#. type: Title ===
#: upstream/_versions/3.2/guides/kafka.adoc:2914
#, no-wrap
msgid "Red Hat OpenShift Streams for Apache Kafka"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2919
msgid "https://cloud.redhat.com/[Red Hat OpenShift Streams for Apache Kafka] provides managed Kafka brokers.  First, follow the instructions from https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/88e1487a-2a14-4b35-85b9-a7a2d67a37f3[Getting started with the `rhoas` CLI for Red Hat OpenShift Streams for Apache Kafka] to create your Kafka broker instance.  Make sure you copied the client id and client secret associated with the _ServiceAccount_ you created."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2921
msgid "Then, you can configure the Quarkus application to connect to the broker as follows:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2930
#, no-wrap
msgid ""
"kafka.bootstrap.servers=<connection url> # <1>\n"
"kafka.security.protocol=SASL_SSL\n"
"kafka.sasl.mechanism=PLAIN\n"
"kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\\n"
"  username=\"${KAFKA_USERNAME}\" \\ # <2>\n"
"  password=\"${KAFKA_PASSWORD}\"; # <3>\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2932
msgid "The connection string, given on the admin console, such as `demo-c--bjsv-ldd-cvavkc-a.bf2.kafka.rhcloud.com:443`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2933
msgid "The kafka username (the client id from the service account)"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2934
msgid "the kafka password (the client secret from the service account)"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2936
msgid "In general, these properties are prefixed using `%prod` to enable them only when running in production mode."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2939
msgid "As explained in https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/88e1487a-2a14-4b35-85b9-a7a2d67a37f3[Getting started with the rhoas CLI for Red Hat OpenShift Streams for Apache Kafka], to use Red Hat OpenShift Streams for Apache Kafka, you must create the topic beforehand, create a _Service Account_, and provide permissions to read and write to your topic from that service account.  The authentication data (client id and secret) relates to the service account, which means you can implement fine-grain permissions and restrict access to the topic."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2941
msgid "When using Kubernetes, it is recommended to set the client id and secret in a Kubernetes secret:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2951
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: kafka-credentials\n"
"stringData:\n"
"  KAFKA_USERNAME: \"...\"\n"
"  KAFKA_PASSWORD: \"...\"\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2954
msgid "To allow your Quarkus application to use that secret, add the following line to the `application.properties` file:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2958
#, no-wrap
msgid "%prod.quarkus.openshift.env.secrets=kafka-credentials\n"
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:2960
#, no-wrap
msgid "Red Hat OpenShift Service Registry"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2964
msgid "https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-service-registry[Red Hat OpenShift Service Registry] provides fully managed service registry for handling Kafka schemas."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2968
msgid "You can follow the instructions from https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/ab1894d1-cae0-4d11-b185-81d62b4aabc7#_60472331-fa00-48ec-a621-bbd039500c7d[Getting started with Red Hat OpenShift Service Registry], or use the `rhoas` CLI to create a new service registry instance:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2972
#, no-wrap
msgid "rhoas service-registry create --name my-schema-registry\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2977
msgid "Make sure to note the _Registry URL_ of the instance created.  For authentication, you can use the same _ServiceAccount_ you created previously.  You need to make sure that it has the necessary permissions to access the service registry."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2979
msgid "For example, using the `rhoas` CLI, you can grant the `MANAGER` role to the service account:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2983
#, no-wrap
msgid "rhoas service-registry role add --role manager --service-account [SERVICE_ACCOUNT_CLIENT_ID]\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2986
msgid "Then, you can configure the Quarkus application to connect to the schema registry as follows:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:2993
#, no-wrap
msgid ""
"mp.messaging.connector.smallrye-kafka.apicurio.registry.url=${RHOAS_SERVICE_REGISTRY_URL} <1>\n"
"mp.messaging.connector.smallrye-kafka.apicurio.auth.service.token.endpoint=${RHOAS_OAUTH_TOKEN_ENDPOINT} <2>\n"
"mp.messaging.connector.smallrye-kafka.apicurio.auth.client.id=${RHOAS_CLIENT_ID} <3>\n"
"mp.messaging.connector.smallrye-kafka.apicurio.auth.client.secret=${RHOAS_CLIENT_ID} <4>\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2995
msgid "The service registry URL, given on the admin console, such as `https://bu98.serviceregistry.rhcloud.com/t/0e95af2c-6e11-475e-82ee-f13bd782df24/apis/registry/v2`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2996
msgid "The OAuth token endpoint URL, such as `https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token`"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2997
msgid "The client id (from the service account)"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:2998
msgid "The client secret (from the service account)"
msgstr ""

#. type: Title ====
#: upstream/_versions/3.2/guides/kafka.adoc:2999
#, no-wrap
msgid "Binding Red Hat OpenShift managed services to Quarkus application using the Service Binding Operator"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:3003
msgid "If your Quarkus application is deployed on a Kubernetes or OpenShift cluster with link:https://github.com/redhat-developer/service-binding-operator[Service Binding Operator] and link:https://github.com/redhat-developer/app-services-operator/tree/main/docs[OpenShift Application Services] operators installed, configurations necessary to access Red Hat OpenShift Streams for Apache Kafka and Service Registry can be injected to the application using xref:deploying-to-kubernetes.adoc#service_binding[Kubernetes Service Binding]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:3006
msgid "In order to set up the Service Binding, you need first to connect OpenShift managed services to your cluster.  For an OpenShift cluster you can follow the instructions from link:https://github.com/redhat-developer/app-services-guides/tree/main/docs/registry/service-binding-registry#connecting-a-kafka-and-service-registry-instance-to-your-openshift-cluster[Connecting a Kafka and Service Registry instance to your OpenShift cluster]."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:3008
msgid "Once you've connected your cluster with the RHOAS Kafka and Service Registry instances, make sure you've granted necessary permissions to the newly created service account."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:3011
msgid "Then, using the xref:deploying-to-kubernetes.adoc#service_binding[Kubernetes Service Binding] extension, you can configure the Quarkus application to generate `ServiceBinding` resources for those services:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:3015
#, no-wrap
msgid "quarkus.kubernetes-service-binding.detect-binding-resources=true\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:3019
#, no-wrap
msgid ""
"quarkus.kubernetes-service-binding.services.kafka.api-version=rhoas.redhat.com/v1alpha1\n"
"quarkus.kubernetes-service-binding.services.kafka.kind=KafkaConnection\n"
"quarkus.kubernetes-service-binding.services.kafka.name=my-kafka\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:3023
#, no-wrap
msgid ""
"quarkus.kubernetes-service-binding.services.serviceregistry.api-version=rhoas.redhat.com/v1alpha1\n"
"quarkus.kubernetes-service-binding.services.serviceregistry.kind=ServiceRegistryConnection\n"
"quarkus.kubernetes-service-binding.services.serviceregistry.name=my-schema-registry\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:3026
msgid "For this example Quarkus build will generate the following `ServiceBinding` resources:"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:3046
#, no-wrap
msgid ""
"apiVersion: binding.operators.coreos.com/v1alpha1\n"
"kind: ServiceBinding\n"
"metadata:\n"
"  name: my-app-kafka\n"
"spec:\n"
"  application:\n"
"    group: apps.openshift.io\n"
"    name: my-app\n"
"    version: v1\n"
"    kind: DeploymentConfig\n"
"  services:\n"
"    - group: rhoas.redhat.com\n"
"      version: v1alpha1\n"
"      kind: KafkaConnection\n"
"      name: my-kafka\n"
"  detectBindingResources: true\n"
"  bindAsFiles: true\n"
"---\n"
msgstr ""

#. type: delimited block -
#: upstream/_versions/3.2/guides/kafka.adoc:3064
#, no-wrap
msgid ""
"apiVersion: binding.operators.coreos.com/v1alpha1\n"
"kind: ServiceBinding\n"
"metadata:\n"
"  name: my-app-serviceregistry\n"
"spec:\n"
"  application:\n"
"    group: apps.openshift.io\n"
"    name: my-app\n"
"    version: v1\n"
"    kind: DeploymentConfig\n"
"  services:\n"
"    - group: rhoas.redhat.com\n"
"      version: v1alpha1\n"
"      kind: ServiceRegistryConnection\n"
"      name: my-schema-registry\n"
"  detectBindingResources: true\n"
"  bindAsFiles: true\n"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:3068
msgid "You can follow xref:deploying-to-kubernetes.adoc#openshift[Deploying to OpenShift] to deploy your application, including generated `ServiceBinding` resources.  The configuration properties necessary to access the Kafka and Schema Registry instances will be injected to the application automatically at deployment."
msgstr ""

#. type: Title ==
#: upstream/_versions/3.2/guides/kafka.adoc:3069
#, no-wrap
msgid "Going further"
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:3073
msgid "This guide has shown how you can interact with Kafka using Quarkus.  It utilizes SmallRye Reactive Messaging to build data streaming applications."
msgstr ""

#. type: Plain text
#: upstream/_versions/3.2/guides/kafka.adoc:3074
msgid "If you want to go further, check the documentation of https://smallrye.io/smallrye-reactive-messaging[SmallRye Reactive Messaging], the implementation used in Quarkus."
msgstr ""
